{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names \n",
    "Spencer Bernardo-Cheng\n",
    "Max Goldstein\n",
    "Robbie Keehan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1789, 1)\n",
      "                                            newy\n",
      "90    [0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2]\n",
      "91    [0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0]\n",
      "92    [0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0]\n",
      "93    [0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2]\n",
      "94    [2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2]\n",
      "...                                          ...\n",
      "1874  [2, 0, 2, 0, 2, 1, 0, 0, 2, 2, 2, 2, 0, 0]\n",
      "1875  [0, 2, 0, 2, 1, 0, 0, 2, 2, 2, 2, 0, 0, 0]\n",
      "1876  [2, 0, 2, 1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0]\n",
      "1877  [0, 2, 1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0]\n",
      "1878  [2, 1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "[1789 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "datafb = pd.read_csv(\"./FB.csv\")\n",
    "datagoogle = pd.read_csv(\"./GOOGL.csv\")\n",
    "datamsft = pd.read_csv(\"./MSFT.csv\")\n",
    "dataam = pd.read_csv(\"./AMZN (1).csv\")\n",
    "datanflx = pd.read_csv(\"./NFLX.csv\")\n",
    "\n",
    "del datafb['Date']\n",
    "del datagoogle['Date']\n",
    "del datamsft['Date']\n",
    "del datanflx['Date']\n",
    "\n",
    "\n",
    "data = pd.concat([datafb, datagoogle, datamsft, dataam, datanflx], axis=1, sort=False)\n",
    "data = data[90:]\n",
    "del data['Change']\n",
    "del data['Gain']\n",
    "del data['Loss']\n",
    "del data['Average Gain']\n",
    "del data['Average Loss']\n",
    "del data['RS']\n",
    "del data['DM+1']\n",
    "del data['DM-1']\n",
    "del data['TR-14']\n",
    "del data['DM+1-14']\n",
    "del data['DM-1+14']\n",
    "del data['dl+1-14']\n",
    "del data['dl-1-14']\n",
    "del data['dl14diff']\n",
    "del data['dl14sum']\n",
    "del data['DX']\n",
    "del data['TR']\n",
    "\n",
    "up = .03\n",
    "down =-.03\n",
    "ydata = data['% Change']\n",
    "ydata = ydata.apply(lambda x:2 if x>=up else (1 if (x< up and x > down) else 0))\n",
    "ydata = pd.DataFrame(ydata)\n",
    "ydata['newy'] = \"\"\n",
    "# created target data in format to match output of multiple timesteps of 14 days ahead\n",
    "# [0,1,2,3,4,5,6,7]\n",
    "# [1,2,3,4,5,6,7,8]\n",
    "for index, row in ydata.iterrows():\n",
    "    temp = np.array(ydata['% Change'][index-90:index-90+14].values)\n",
    "    ydata['newy'].at[index] = temp\n",
    "    \n",
    "# removed last 14 because their targets are not able to created\n",
    "ydata = ydata[0:-13]\n",
    "data = data[0:-13]\n",
    "del ydata['% Change']\n",
    "print(ydata.shape)\n",
    "print(ydata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding The Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          newy\n",
      "90  [0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2]\n",
      "91  [0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0]\n",
      "92  [0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0]\n",
      "93  [0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2]\n",
      "94  [2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2]\n",
      "                                                   newy\n",
      "90    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...\n",
      "91    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...\n",
      "92    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "93    [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, ...\n",
      "94    [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "...                                                 ...\n",
      "1874  [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "1875  [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, ...\n",
      "1876  [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "1877  [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, ...\n",
      "1878  [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, ...\n",
      "\n",
      "[1789 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "print(ydata.head())\n",
    "ydata['newy'] = ydata['newy'].apply(lambda x: to_categorical(x,3))\n",
    "print(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Scaling the Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00706588 0.00746643 0.00598557 ... 0.00188326 0.00188326 0.42421594]\n",
      " [0.00472753 0.00440971 0.00332533 ... 0.00187282 0.00187282 0.19557866]\n",
      " [0.00447336 0.00450992 0.00414387 ... 0.00203991 0.00203991 0.10745063]\n",
      " ...\n",
      " [0.88415002 0.87677893 0.88325571 ... 0.68480541 0.68480541 0.02587954]\n",
      " [0.89091091 0.88314295 0.89312939 ... 0.69798828 0.69798828 0.02485125]\n",
      " [0.89243593 0.89982967 0.89144114 ... 0.71636145 0.71636145 0.03687678]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "del data['Date']\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(data.iloc[:,:].values)\n",
    "print(x_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Time Series Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Lambda\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "n_input = 2\n",
    "series = np.array(x_scaled)\n",
    "target = np.array(ydata)\n",
    "generator = TimeseriesGenerator(series, target, length=90, batch_size=1)\n",
    "xtime = []\n",
    "ytarget =[]\n",
    "for i in range(len(generator)):\n",
    "    x, y = generator[i]\n",
    "    xtime.append(x[0])\n",
    "    ytarget.append(y[0])\n",
    "xtime = np.array(xtime)\n",
    "ytarget = np.array(ytarget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Fold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1359, 90, 36)\n",
      "[[array([[0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.]], dtype=float32)]\n",
      " [array([[0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32)]\n",
      " [array([[1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.]], dtype=float32)]\n",
      " ...\n",
      " [array([[0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32)]\n",
      " [array([[0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32)]\n",
      " [array([[0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "x_train,x_test,y_train,y_test = train_test_split(xtime, ytarget, test_size = 1/5, random_state = 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 90, 100)           54800     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 14, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 14, 3)             303       \n",
      "=================================================================\n",
      "Total params: 55,103\n",
      "Trainable params: 55,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_13 to have 3 dimensions, but got array with shape (1359, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-1fa753146e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_13 to have 3 dimensions, but got array with shape (1359, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(90, 36), return_sequences=True))\n",
    "model.add(Lambda(lambda x: x[:, -14:, :]))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(x_train, y_train, validation_data=(x_test,y_test), shuffle=True, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
