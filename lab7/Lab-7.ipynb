{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names \n",
    "Spencer Bernardo-Cheng\n",
    "Max Goldstein\n",
    "Robbie Keehan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Open-fb   High-fb    Low-fb  Close-fb  Adj Close-fb  Volume-fb  \\\n",
      "90   -0.459999 -0.609998 -0.519998 -0.590001     -0.590001   12160000   \n",
      "91   -0.050001  0.019998  0.160000  0.110001      0.110001  -17504500   \n",
      "92   -0.129999 -0.160000 -0.130001 -0.230000     -0.230000   -3007900   \n",
      "93   -0.070000  0.080000  0.010000  0.000000      0.000000    1380300   \n",
      "94    0.000000 -0.189998 -0.190001 -0.040000     -0.040000    1645000   \n",
      "...        ...       ...       ...       ...           ...        ...   \n",
      "1873  0.070007  4.979996  1.200012  4.860000      4.860000    8984300   \n",
      "1874  4.699997  0.639999  3.000000 -1.279999     -1.279999   -6754800   \n",
      "1875 -1.770004 -1.669998 -1.290008 -0.040008     -0.040008   -1820200   \n",
      "1876  1.330002  1.270004  1.930008  1.950012      1.950012    2483800   \n",
      "1877  0.300003  3.330002 -0.330002  2.299988      2.299988    4642900   \n",
      "\n",
      "      Open-googl  High-googl  Low-googl  Close-googl  ...   Low-amzn  \\\n",
      "90     -8.913910   -6.901916  -2.122101     0.235230  ...  -6.720001   \n",
      "91      5.525513    5.490509   6.005982     3.463470  ...  -2.119996   \n",
      "92     -0.525513   -1.816834  -3.098084    -3.368377  ...   0.020005   \n",
      "93     -4.959961   -5.525513  -6.706695    -1.886872  ...  -3.400009   \n",
      "94     -0.905914    1.581574   2.882873     1.861847  ...   3.570007   \n",
      "...          ...         ...        ...          ...  ...        ...   \n",
      "1873   -5.430054    4.449952  -1.630005    -1.070068  ...   4.780029   \n",
      "1874   -5.389892   -6.829956  -2.089966    -1.029907  ... -24.590088   \n",
      "1875    1.819946   13.199951   1.789917    12.969970  ...   2.240113   \n",
      "1876   20.050049   17.820068  17.950074    24.390015  ... -16.700074   \n",
      "1877   17.289917    0.380005   4.270019   -13.700073  ... -10.150024   \n",
      "\n",
      "      Close-amzn  Adj Close-amzn  Volume-amzn     SMA-7    SMA-21    SMA-90  \\\n",
      "90     -5.970002       -5.970002       453500 -1.002856 -0.508571  0.338000   \n",
      "91     -0.770004       -0.770004      -501000 -0.911429 -0.543333  0.344555   \n",
      "92     -1.860000       -1.860000     -1244100 -1.937142 -0.851428  0.274667   \n",
      "93      1.819992        1.819992       756600 -2.327144 -0.813809  0.282000   \n",
      "94     -0.239991       -0.239991      -755700 -2.081430 -0.669524  0.282889   \n",
      "...          ...             ...          ...       ...       ...       ...   \n",
      "1873    6.349976        6.349976        91600 -1.919992  1.979522 -1.936889   \n",
      "1874  -24.890015      -24.890015       951900 -7.364292 -0.679525 -2.613223   \n",
      "1875    1.489991        1.489991      -724700 -6.729998 -1.087147 -2.920111   \n",
      "1876  -15.109986      -15.109986      1662800 -8.040005 -2.285237 -2.906444   \n",
      "1877   13.040039       13.040039     -1088100 -5.095703 -0.237142 -2.871889   \n",
      "\n",
      "      % Change  RSI-14 Day        ADX  \n",
      "90   -3.126685   47.267575  16.420508  \n",
      "91   -2.378866   41.102218  14.497504  \n",
      "92   -0.314300   40.370801  13.151813  \n",
      "93   -0.761608   38.584645  12.713275  \n",
      "94    0.750946   41.320419  13.057054  \n",
      "...        ...         ...        ...  \n",
      "1873 -0.796805   48.562974  13.738852  \n",
      "1874  0.358422   50.465834  13.738689  \n",
      "1875 -1.399888   43.649539  15.293906  \n",
      "1876  0.084991   44.135976  13.924882  \n",
      "1877 -0.861164   40.333553  15.356140  \n",
      "\n",
      "[1788 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "datafb = pd.read_csv(\"./FB.csv\")\n",
    "datagoogle = pd.read_csv(\"./GOOGL.csv\")\n",
    "datamsft = pd.read_csv(\"./MSFT.csv\")\n",
    "dataam = pd.read_csv(\"./AMZN (1).csv\")\n",
    "datanflx = pd.read_csv(\"./NFLX.csv\")\n",
    "\n",
    "del datafb['Date']\n",
    "del datagoogle['Date']\n",
    "del datamsft['Date']\n",
    "del datanflx['Date']\n",
    "\n",
    "\n",
    "data = pd.concat([datafb, datagoogle, datamsft,datanflx, dataam ], axis=1, sort=False)\n",
    "data = data[90:]\n",
    "del data['Change']\n",
    "del data['Gain']\n",
    "del data['Loss']\n",
    "del data['Average Gain']\n",
    "del data['Average Loss']\n",
    "del data['RS']\n",
    "del data['DM+1']\n",
    "del data['DM-1']\n",
    "del data['TR-14']\n",
    "del data['DM+1-14']\n",
    "del data['DM-1+14']\n",
    "del data['dl+1-14']\n",
    "del data['dl-1-14']\n",
    "del data['dl14diff']\n",
    "del data['dl14sum']\n",
    "del data['DX']\n",
    "del data['TR']\n",
    "\n",
    "predictedDays = 14\n",
    "up = .03\n",
    "down =-.03\n",
    "ydata = data['% Change']\n",
    "ydata = ydata.apply(lambda x:2 if x>=up else (1 if (x< up and x > down) else 0))\n",
    "ydata = pd.DataFrame(ydata)\n",
    "ydata['newy'] = \"\"\n",
    "# created target data in format to match output of multiple timesteps of 14 days ahead\n",
    "# [0,1,2,3,4,5,6,7]\n",
    "# [1,2,3,4,5,6,7,8]\n",
    "for index, row in ydata.iterrows():\n",
    "    temp = np.array(ydata['% Change'][index-90:index-90+predictedDays].values)\n",
    "    ydata['newy'].at[index] = temp\n",
    "    \n",
    "# removed last 14 because their targets are not able to created\n",
    "ydata = ydata[0:-(predictedDays-1)]\n",
    "data = data[0:-(predictedDays-1)]\n",
    "del ydata['% Change']\n",
    "del data['Date']\n",
    "\n",
    "colheaders = data.columns[0:-3]\n",
    "\n",
    "for header in colheaders: \n",
    "    for i in range(1, len(data)):\n",
    "        data[header].at[90+i-1]= data[header].iloc[i] - data[header].iloc[i - 1]\n",
    "        \n",
    "data = data[0:-1]\n",
    "ydata= ydata[0:-1]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding The Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          newy\n",
      "90  [0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2]\n",
      "91  [0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0]\n",
      "92  [0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0]\n",
      "93  [0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2]\n",
      "94  [2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2]\n",
      "                                                   newy\n",
      "90    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...\n",
      "91    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...\n",
      "92    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "93    [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, ...\n",
      "94    [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "...                                                 ...\n",
      "1873  [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, ...\n",
      "1874  [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "1875  [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, ...\n",
      "1876  [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, ...\n",
      "1877  [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, ...\n",
      "\n",
      "[1788 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "print(ydata.head())\n",
    "ydata['newy'] = ydata['newy'].apply(lambda x: to_categorical(x,3))\n",
    "print(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Scaling the Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6704866  0.63964878 0.66867271 ... 0.31321402 0.35811382 0.15022488]\n",
      " [0.67729607 0.65028702 0.68004011 ... 0.34297397 0.26414926 0.12331708]\n",
      " [0.67596742 0.64724754 0.67519221 ... 0.42513468 0.25300192 0.10448737]\n",
      " ...\n",
      " [0.64872934 0.62174942 0.65580059 ... 0.38193301 0.3029723  0.1344608 ]\n",
      " [0.7002159  0.67139486 0.709629   ... 0.44102474 0.31038595 0.1153046 ]\n",
      " [0.68310912 0.70618038 0.67184883 ... 0.40337188 0.25243424 0.13533161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(data.iloc[:,:].values)\n",
    "print(x_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Time Series Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Lambda\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "n_input = 2\n",
    "series = np.array(x_scaled)\n",
    "target = np.array(ydata)\n",
    "generator = TimeseriesGenerator(series, target, length=90, batch_size=1)\n",
    "xtime = []\n",
    "ytarget =[]\n",
    "for i in range(len(generator)):\n",
    "    x, y = generator[i]\n",
    "    xtime.append(x[0])\n",
    "    ytarget.append(y[0][0])\n",
    "xtime = np.array(xtime)\n",
    "ytarget = np.array(ytarget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Fold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1273, 90, 36)\n",
      "(1273, 14, 3)\n",
      "(425, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "x_train,x_test,y_train,y_test = train_test_split(xtime, ytarget, test_size = 1/4, random_state = 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 90, 100)           54800     \n",
      "_________________________________________________________________\n",
      "lambda_16 (Lambda)           (None, 14, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 14, 3)             303       \n",
      "=================================================================\n",
      "Total params: 55,103\n",
      "Trainable params: 55,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1273 samples, validate on 425 samples\n",
      "Epoch 1/200\n",
      "1273/1273 [==============================] - 7s 6ms/step - loss: 1.0486 - acc: 0.5159 - val_loss: 0.8354 - val_acc: 0.5345\n",
      "Epoch 2/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.8448 - acc: 0.5190 - val_loss: 0.8169 - val_acc: 0.5345\n",
      "Epoch 3/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7844 - acc: 0.5306 - val_loss: 0.7809 - val_acc: 0.5345\n",
      "Epoch 4/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7729 - acc: 0.5277 - val_loss: 0.7757 - val_acc: 0.5345\n",
      "Epoch 5/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7714 - acc: 0.5271 - val_loss: 0.7786 - val_acc: 0.5346\n",
      "Epoch 6/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7715 - acc: 0.5275 - val_loss: 0.7767 - val_acc: 0.5345\n",
      "Epoch 7/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7704 - acc: 0.5314 - val_loss: 0.7741 - val_acc: 0.5345\n",
      "Epoch 8/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7693 - acc: 0.5290 - val_loss: 0.7788 - val_acc: 0.4489\n",
      "Epoch 9/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7699 - acc: 0.5264 - val_loss: 0.7769 - val_acc: 0.5345\n",
      "Epoch 10/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7695 - acc: 0.5314 - val_loss: 0.7756 - val_acc: 0.5345\n",
      "Epoch 11/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7687 - acc: 0.5287 - val_loss: 0.7737 - val_acc: 0.5345\n",
      "Epoch 12/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7691 - acc: 0.5302 - val_loss: 0.7732 - val_acc: 0.5345\n",
      "Epoch 13/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7688 - acc: 0.5307 - val_loss: 0.7722 - val_acc: 0.5345\n",
      "Epoch 14/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7688 - acc: 0.5288 - val_loss: 0.7726 - val_acc: 0.5345\n",
      "Epoch 15/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7694 - acc: 0.5317 - val_loss: 0.7729 - val_acc: 0.5345\n",
      "Epoch 16/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7686 - acc: 0.5327 - val_loss: 0.7726 - val_acc: 0.5345\n",
      "Epoch 17/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7683 - acc: 0.5287 - val_loss: 0.7723 - val_acc: 0.5345\n",
      "Epoch 18/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7680 - acc: 0.5318 - val_loss: 0.7732 - val_acc: 0.5311\n",
      "Epoch 19/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7686 - acc: 0.5320 - val_loss: 0.7729 - val_acc: 0.5345\n",
      "Epoch 20/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7689 - acc: 0.5306 - val_loss: 0.7725 - val_acc: 0.5345\n",
      "Epoch 21/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7690 - acc: 0.5315 - val_loss: 0.7722 - val_acc: 0.5323\n",
      "Epoch 22/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7678 - acc: 0.5317 - val_loss: 0.7727 - val_acc: 0.5345\n",
      "Epoch 23/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7815 - acc: 0.5323 - val_loss: 0.7729 - val_acc: 0.5351\n",
      "Epoch 24/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7765 - acc: 0.5306 - val_loss: 0.7727 - val_acc: 0.5329\n",
      "Epoch 25/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7686 - acc: 0.5314 - val_loss: 0.7746 - val_acc: 0.5361\n",
      "Epoch 26/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 2.0435 - acc: 0.5215 - val_loss: 8.5927 - val_acc: 0.4669\n",
      "Epoch 27/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.3806 - acc: 0.4759 - val_loss: 8.4708 - val_acc: 0.4745\n",
      "Epoch 28/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.4678 - acc: 0.4743 - val_loss: 8.6713 - val_acc: 0.4620\n",
      "Epoch 29/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.4993 - acc: 0.4717 - val_loss: 8.5141 - val_acc: 0.4718\n",
      "Epoch 30/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 3.4860 - acc: 0.5065 - val_loss: 0.7718 - val_acc: 0.5351\n",
      "Epoch 31/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.8126 - acc: 0.5322 - val_loss: 0.7722 - val_acc: 0.5351\n",
      "Epoch 32/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 3.6080 - acc: 0.5116 - val_loss: 2.6409 - val_acc: 0.5292\n",
      "Epoch 33/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5086 - acc: 0.4976 - val_loss: 8.3714 - val_acc: 0.4805\n",
      "Epoch 34/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.3093 - acc: 0.4791 - val_loss: 8.3543 - val_acc: 0.4817\n",
      "Epoch 35/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.6001 - acc: 0.4870 - val_loss: 7.5471 - val_acc: 0.5318\n",
      "Epoch 36/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.3746 - acc: 0.5291 - val_loss: 7.5209 - val_acc: 0.5333\n",
      "Epoch 37/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.2429 - acc: 0.5310 - val_loss: 7.5281 - val_acc: 0.5329\n",
      "Epoch 38/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5380 - acc: 0.5310 - val_loss: 7.5146 - val_acc: 0.5338\n",
      "Epoch 39/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5106 - acc: 0.5318 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 40/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5043 - acc: 0.5329 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 41/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 5.7987 - acc: 0.5337 - val_loss: 7.0011 - val_acc: 0.5345\n",
      "Epoch 42/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 4.3037 - acc: 0.5332 - val_loss: 0.7736 - val_acc: 0.5345\n",
      "Epoch 43/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.8131 - acc: 0.5335 - val_loss: 0.7724 - val_acc: 0.5351\n",
      "Epoch 44/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.8027 - acc: 0.5325 - val_loss: 0.7722 - val_acc: 0.5345\n",
      "Epoch 45/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.8404 - acc: 0.5324 - val_loss: 0.7724 - val_acc: 0.5345\n",
      "Epoch 46/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7851 - acc: 0.5327 - val_loss: 0.7725 - val_acc: 0.5351\n",
      "Epoch 47/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7671 - acc: 0.5314 - val_loss: 0.7761 - val_acc: 0.5331\n",
      "Epoch 48/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7696 - acc: 0.5333 - val_loss: 0.7711 - val_acc: 0.5345\n",
      "Epoch 49/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7974 - acc: 0.5330 - val_loss: 0.7717 - val_acc: 0.5338\n",
      "Epoch 50/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7669 - acc: 0.5322 - val_loss: 0.7720 - val_acc: 0.5345\n",
      "Epoch 51/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 0.7665 - acc: 0.5333 - val_loss: 0.7721 - val_acc: 0.5329\n",
      "Epoch 52/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 4.4336 - acc: 0.5290 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 53/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.3285 - acc: 0.4976 - val_loss: 8.4464 - val_acc: 0.4760\n",
      "Epoch 54/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.1153 - acc: 0.4930 - val_loss: 8.4112 - val_acc: 0.4782\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.1451 - acc: 0.4946 - val_loss: 8.2216 - val_acc: 0.4899\n",
      "Epoch 56/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.2012 - acc: 0.4903 - val_loss: 8.0211 - val_acc: 0.5024\n",
      "Epoch 57/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.9650 - acc: 0.5058 - val_loss: 8.0320 - val_acc: 0.5017\n",
      "Epoch 58/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 8.1259 - acc: 0.4955 - val_loss: 8.2785 - val_acc: 0.4864\n",
      "Epoch 59/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.6021 - acc: 0.5281 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 60/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 61/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 62/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 63/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 64/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 65/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 66/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 67/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 68/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 69/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 70/200\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 7.5327 - acc: 0.5327 - val_loss: 7.5037 - val_acc: 0.5345\n",
      "Epoch 71/200\n",
      "1216/1273 [===========================>..] - ETA: 0s - loss: 7.5317 - acc: 0.5327"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-cb6c7c23429a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, dropout=.2,activation='relu', input_shape=(90, 36), return_sequences=True))\n",
    "model.add(Lambda(lambda x: x[:, -predictedDays:, :]))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit(x_train, y_train, validation_data=(x_test,y_test), shuffle=True, epochs=200, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "#From Sklearn documentation \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = list(unique_labels(y_true, y_pred))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['down','neutral','up']\n",
    "yhat = model.predict(x_test)\n",
    "\n",
    "ytest2d = y_test.reshape((predictedDays*428,3))\n",
    "yhat2d = yhat.reshape((predictedDays*428,3))\n",
    "\n",
    "ytest2d = ytest2d.argmax(axis=1)\n",
    "yhat2d=yhat2d.argmax(axis=1)\n",
    "plot_confusion_matrix(ytest2d, yhat2d, classes=class_names,\n",
    "                      title='SciKit Learn Implementation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test[243][np.newaxis,:,:])\n",
    "print(x_test[0].shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
