{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hU5HQKgsCpnL"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYUjDzZ-CpnJ",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2\n",
        "    -Robbie Keehan\n",
        "    -Spencer Cheng\n",
        "    -Max Goldstein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsF0Fmt9CtjZ",
        "colab_type": "code",
        "outputId": "1c12f091-8c17-42c2-ee2e-39914eac15d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJnzKxtrjU7_",
        "colab_type": "text"
      },
      "source": [
        "Chose the valadation set of coco because it is smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU5HQKgsCpnL",
        "colab_type": "text"
      },
      "source": [
        "## Make VGG use unpooling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF62arYvCpnM",
        "colab_type": "text"
      },
      "source": [
        "This is taken directly from the example. Not sure if I did strided stuff right. Also unsure if we are even supposed to be changing VGG like this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoYkUVgoSiyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
        "from keras.utils.data_utils import get_file\n",
        "import tensorflow.keras.backend as K\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
        "\n",
        "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                        WEIGHTS_PATH_NO_TOP,\n",
        "                        cache_subdir='models',\n",
        "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
        "\n",
        "def vgg_layers(inputs, target_layer):\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
        "    if target_layer == 1:\n",
        "        return x\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    if target_layer == 2:\n",
        "        return x\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    if target_layer == 3:\n",
        "        return x\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    if target_layer == 4:\n",
        "        return x\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def load_weights(model):\n",
        "    # f = h5py.File(WEIGHTS_PATH)\n",
        "    # layer_names = [name for name in f.attrs['layer_names']]\n",
        "\n",
        "    for layer in model.layers:\n",
        "        # b_name = layer.name.encode()\n",
        "        # if b_name in layer_names:\n",
        "        #     g = f[b_name]\n",
        "        #     weights = [g[name] for name in g.attrs['weight_names']]\n",
        "        #     layer.set_weights(weights)\n",
        "        layer.trainable = False\n",
        "\n",
        "    # f.close()\n",
        "\n",
        "\n",
        "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
        "    \"\"\"\n",
        "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
        "    \"\"\"\n",
        "    if input_tensor is None:\n",
        "        inputs = Input(shape=input_shape)\n",
        "    else:\n",
        "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
        "    model = tf.keras.applications.vgg19.VGG19()\n",
        "    load_weights(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_input(x):\n",
        "    # Convert 'RGB' -> 'BGR'\n",
        "    if type(x) is np.ndarray:\n",
        "        x = x[..., ::-1]\n",
        "    else:\n",
        "        x = tf.reverse(x, [-1])\n",
        "\n",
        "    return x - MEAN_PIXEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7zVRaXGCpnR",
        "colab_type": "text"
      },
      "source": [
        "# Larson Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wXlnzrSq4OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Conv2D, UpSampling2D\n",
        "\n",
        "def decoder_layers(inputs, layer):\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
        "    if layer == 1:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
        "    if layer == 2:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
        "    if layer == 3:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
        "    if layer == 4:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
        "    if layer == 5:\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8oTp-ALCpnY",
        "colab_type": "text"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdK0QkrCpnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing.pool\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n",
        "    \"\"\"Count files with extension in `white_list_formats` contained in a directory.\n",
        "\n",
        "    # Arguments\n",
        "        directory: absolute path to the directory containing files to be counted\n",
        "        white_list_formats: set of strings containing allowed extensions for\n",
        "            the files to be counted.\n",
        "\n",
        "    # Returns\n",
        "        the count of files with extension in `white_list_formats` contained in\n",
        "        the directory.\n",
        "    \"\"\"\n",
        "    def _recursive_list(subpath):\n",
        "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
        "\n",
        "    samples = 0\n",
        "    for root, _, files in _recursive_list(directory):\n",
        "        for fname in files:\n",
        "            is_valid = False\n",
        "            for extension in white_list_formats:\n",
        "                if fname.lower().endswith('.' + extension):\n",
        "                    is_valid = True\n",
        "                    break\n",
        "            if is_valid:\n",
        "                samples += 1\n",
        "    return samples\n",
        "\n",
        "def count_num_samples(directory):\n",
        "    \"\"\"\n",
        "    From Keras DirectoryIterator\n",
        "    \"\"\"\n",
        "    classes = []\n",
        "    for subdir in sorted(os.listdir(directory)):\n",
        "        if os.path.isdir(os.path.join(directory, subdir)):\n",
        "            classes.append(subdir)\n",
        "\n",
        "    white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm'}\n",
        "    pool = multiprocessing.pool.ThreadPool()\n",
        "    function_partial = partial(_count_valid_files_in_directory,\n",
        "                               white_list_formats=white_list_formats,\n",
        "                               follow_links=False)\n",
        "    num_samples = sum(pool.map(function_partial,\n",
        "                               (os.path.join(directory, subdir)\n",
        "                                for subdir in classes)))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return num_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3NeYGJGCpng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.callbacks import Callback\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import imageio\n",
        "import glob\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc2lr_utCpnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "def resize_img(img, size):\n",
        "    img = np.copy(img)\n",
        "    factors = (1,\n",
        "               float(size[0]) / img.shape[1],\n",
        "               float(size[1]) / img.shape[2],\n",
        "               1)\n",
        "    return scipy.ndimage.zoom(img, factors, order=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-D4-h2SzYF",
        "colab_type": "text"
      },
      "source": [
        "# Reconstruct Images from Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmqDSL8hq9VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.models import Model, Sequential, load_model\n",
        "# from  tensorflow.keras.layers import Conv2D, Input\n",
        "# import  tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "# LAMBDA=1\n",
        "\n",
        "# def l2_loss(x):\n",
        "#     return K.sum(K.square(x)) / 2\n",
        "\n",
        "# class EncoderDecoder:\n",
        "#     def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
        "#                  decoder_path=None):\n",
        "#         self.input_shape = input_shape\n",
        "#         self.target_layer = target_layer\n",
        "\n",
        "#         self.encoder = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
        "#         if decoder_path:\n",
        "#             self.decoder = tf.keras.models.load_model(decoder_path)\n",
        "#         else:\n",
        "#             self.decoder = self.create_decoder(target_layer)\n",
        "\n",
        "#         self.model = tf.keras.Sequential()\n",
        "#         self.model.add(self.encoder)\n",
        "#         print(self.encoder)\n",
        "#         print(self.model)\n",
        "#         print(self.decoder)\n",
        "#         self.model.add(self.decoder)\n",
        "\n",
        "#         self.loss = self.create_loss_fn(self.encoder)\n",
        "\n",
        "#         self.model.compile('adam', self.loss)\n",
        "\n",
        "#     def create_loss_fn(self, encoder):\n",
        "#         def get_encodings(inputs):\n",
        "#             encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
        "#             return encoder.output\n",
        "\n",
        "#         def loss(img_in, img_out):\n",
        "#             encoding_in = get_encodings(img_in)\n",
        "#             encoding_out = get_encodings(img_out)\n",
        "#             return l2_loss(img_out - img_in) + \\\n",
        "#                    LAMBDA*l2_loss(encoding_out - encoding_in)\n",
        "#         return loss\n",
        "\n",
        "#     def create_decoder(self, target_layer):\n",
        "#         inputs = Input(shape=self.encoder.output_shape[1:])\n",
        "#         layers = decoder_layers(inputs, target_layer)\n",
        "#         output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
        "#                         name='decoder_out')(layers)\n",
        "#         return Model(inputs, output, name='decoder_%s' % target_layer)\n",
        "\n",
        "#     def export_decoder(self):\n",
        "#         self.decoder.save('decoder_%s.h5' % self.target_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT639SLWV_Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
        "from keras.utils.data_utils import get_file\n",
        "import keras.backend as K\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
        "\n",
        "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                        WEIGHTS_PATH_NO_TOP,\n",
        "                        cache_subdir='models',\n",
        "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
        "\n",
        "def vgg_layers(inputs, target_layer):\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
        "    if target_layer == 1:\n",
        "        return x\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    if target_layer == 2:\n",
        "        return x\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    if target_layer == 3:\n",
        "        return x\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    if target_layer == 4:\n",
        "        return x\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def load_weights(model):\n",
        "    f = h5py.File(WEIGHTS_PATH)\n",
        "    layer_names = [name for name in f.attrs['layer_names']]\n",
        "\n",
        "    for layer in model.layers:\n",
        "        b_name = layer.name.encode()\n",
        "        if b_name in layer_names:\n",
        "            g = f[b_name]\n",
        "            weights = [g[name] for name in g.attrs['weight_names']]\n",
        "            layer.set_weights(weights)\n",
        "            layer.trainable = False\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
        "    \"\"\"\n",
        "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
        "    \"\"\"\n",
        "    if input_tensor is None:\n",
        "        inputs = Input(shape=input_shape)\n",
        "    else:\n",
        "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
        "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19')\n",
        "    load_weights(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_input(x):\n",
        "    # Convert 'RGB' -> 'BGR'\n",
        "    if type(x) is np.ndarray:\n",
        "        x = x[..., ::-1]\n",
        "    else:\n",
        "        x = tf.reverse(x, [-1])\n",
        "\n",
        "    return x - MEAN_PIXEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpR4n6NqVw8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Conv2D, Input\n",
        "import keras.backend as K\n",
        "\n",
        "LAMBDA=1\n",
        "\n",
        "def l2_loss(x):\n",
        "    return K.sum(K.square(x)) / 2\n",
        "\n",
        "class EncoderDecoder:\n",
        "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
        "                 decoder_path=None):\n",
        "        self.input_shape = input_shape\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.encoder = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
        "        if decoder_path:\n",
        "            self.decoder = load_model(decoder_path)\n",
        "        else:\n",
        "            self.decoder = self.create_decoder(target_layer)\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(self.encoder)\n",
        "        self.model.add(self.decoder)\n",
        "\n",
        "        self.loss = self.create_loss_fn(self.encoder)\n",
        "\n",
        "        self.model.compile('adam', self.loss)\n",
        "\n",
        "    def create_loss_fn(self, encoder):\n",
        "        def get_encodings(inputs):\n",
        "            encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
        "            return encoder.output\n",
        "\n",
        "        def loss(img_in, img_out):\n",
        "            encoding_in = get_encodings(img_in)\n",
        "            encoding_out = get_encodings(img_out)\n",
        "            return l2_loss(img_out - img_in) + \\\n",
        "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
        "        return loss\n",
        "\n",
        "    def create_decoder(self, target_layer):\n",
        "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
        "        layers = decoder_layers(inputs, target_layer)\n",
        "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
        "                        name='decoder_out')(layers)\n",
        "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
        "\n",
        "    def export_decoder(self):\n",
        "        self.decoder.save('decoder_%s.h5' % self.target_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpbT416Kpog",
        "colab_type": "code",
        "outputId": "1868e5a4-711e-4e89-a03c-3230f22ee058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DECODER_PATH = './decoder_4-ep3.h5'\n",
        "INPUT_IMG_PATH = './download.jpg'\n",
        "OUTPUT_IMG_PATH = './decoded-1.jpg'\n",
        "encoder_decoder = EncoderDecoder(decoder_path=DECODER_PATH)\n",
        "\n",
        "input_img = load_img(INPUT_IMG_PATH)\n",
        "input_img = img_to_array(input_img)\n",
        "# input_img = resize_img(input_img, (256, 256, 3))\n",
        "input_img = np.expand_dims(input_img, axis=0)\n",
        "\n",
        "output_img = encoder_decoder.model.predict([input_img])[0]\n",
        "imageio.imwrite(OUTPUT_IMG_PATH, output_img)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0.0, 16.794511795043945]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDiGJY8Aq2R0",
        "colab_type": "code",
        "outputId": "614abfe2-2f20-4ddd-9eef-8b1a2bb84005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "TRAIN_PATH = './images/'\n",
        "TARGET_SIZE = (256, 256)\n",
        "BATCH_SIZE = 8\n",
        "epochs = 0\n",
        "\n",
        "def create_gen(img_dir, target_size, batch_size):\n",
        "    datagen = ImageDataGenerator()\n",
        "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
        "                                      batch_size=batch_size, class_mode=None)\n",
        "\n",
        "    def tuple_gen():\n",
        "        for img in gen:\n",
        "            if img.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            # (X, y)\n",
        "            yield (img, img)\n",
        "\n",
        "    return tuple_gen()\n",
        "\n",
        "# This needs to be in scope where model is defined\n",
        "class OutputPreview(Callback):\n",
        "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
        "        test_img = load_img(test_img_path)\n",
        "        test_img = img_to_array(test_img)\n",
        "        test_target = resize_img(test_img, (256, 256, 3))\n",
        "        test_target = np.expand_dims(test_target, axis=0)\n",
        "        self.test_img = test_target\n",
        "        self.model = model\n",
        "        self.preview_dir_path = preview_dir_path\n",
        "        self.increment = increment\n",
        "        self.iteration = 0\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        if (self.iteration % self.increment == 0):\n",
        "            output_img = self.model.predict(self.test_img)[0]\n",
        "            fname = '%d.jpg' % self.iteration\n",
        "            out_path = os.path.join(self.preview_dir_path, fname)\n",
        "            imageio.imwrite(out_path, output_img)\n",
        "\n",
        "        self.iteration += 1\n",
        "\n",
        "\n",
        "gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
        "\n",
        "num_samples = count_num_samples(TRAIN_PATH)\n",
        "steps_per_epoch = num_samples // BATCH_SIZE\n",
        "\n",
        "target_layer = int(4)\n",
        "\n",
        "encoder_decoder = EncoderDecoder(target_layer=target_layer,decoder_path='./decoder_4-ep3.h5')\n",
        "\n",
        "# callbacks = [OutputPreview(encoder_decoder, './download.jpg', 40504, './preview-%d' % target_layer)]\n",
        "encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs)\n",
        "encoder_decoder.export_decoder()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0b792f81589c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtarget_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mencoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./decoder_4-ep3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# callbacks = [OutputPreview(encoder_decoder, './download.jpg', 40504, './preview-%d' % target_layer)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-43565b42e127>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, target_layer, decoder_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise TypeError('The added layer must be '\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.engine.training.Model object at 0x7ff35e63ea58>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xsQ3CAQS40k",
        "colab_type": "text"
      },
      "source": [
        "# Apply Whitening & Color Transform "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu4llrIdPMWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "content_url = './download.jpg'\n",
        "style_url='./starry.jpeg'\n",
        "OUTPUT_IMG_PATH = './style-transfer-1.jpg'\n",
        "\n",
        "\n",
        "def get_vgg_features(vgg, inputs, target_layer):\n",
        "    output_layers = [\n",
        "            'block1_conv1',\n",
        "            'block2_conv1',\n",
        "            'block3_conv1',\n",
        "            'block4_conv1',\n",
        "            'block5_conv1'\n",
        "    ]\n",
        "\n",
        "    outputs = [layer.output for layer in vgg.layers\n",
        "               if layer.name == output_layers[target_layer-1]]\n",
        "    f = K.function([vgg.input] + [K.learning_phase()], outputs)\n",
        "    return f([inputs, 1])\n",
        "\n",
        "\n",
        "def wct(content, style, alpha=0.6, eps=1e-5):\n",
        "    '''\n",
        "    https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
        "\n",
        "       Perform Whiten-Color Transform on feature maps using numpy\n",
        "       See p.4 of the Universal Style Transfer paper for equations:\n",
        "       thttps://arxiv.org/pdf/1705.08086.pdf\n",
        "    '''\n",
        "    # 1xHxWxC -> CxHxW\n",
        "    content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
        "    style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
        "\n",
        "    # CxHxW -> CxH*W\n",
        "    content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
        "    style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
        "\n",
        "    # Whitening transform\n",
        "    mc = content_flat.mean(axis=1, keepdims=True)\n",
        "    fc = content_flat - mc\n",
        "    fcfc = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
        "    Ec, wc, _ = np.linalg.svd(fcfc)\n",
        "    k_c = (wc > 1e-5).sum()\n",
        "    Dc = np.diag((wc[:k_c]+eps)**-0.5)\n",
        "    fc_hat = Ec[:,:k_c].dot(Dc).dot(Ec[:,:k_c].T).dot(fc)\n",
        "\n",
        "    # Coloring transform\n",
        "    ms = style_flat.mean(axis=1, keepdims=True)\n",
        "    fs = style_flat - ms\n",
        "    fsfs = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
        "    Es, ws, _ = np.linalg.svd(fsfs)\n",
        "    k_s = (ws > 1e-5).sum()\n",
        "    Ds = np.sqrt(np.diag(ws[:k_s]+eps))\n",
        "    fcs_hat = Es[:,:k_s].dot(Ds).dot(Es[:,:k_s].T).dot(fc_hat)\n",
        "    fcs_hat = fcs_hat + ms\n",
        "\n",
        "    # Blend transform features with original features\n",
        "    blended = alpha*fcs_hat + (1 - alpha)*(fc)\n",
        "\n",
        "    # CxH*W -> CxHxW\n",
        "    blended = blended.reshape(content_t.shape)\n",
        "    # CxHxW -> 1xHxWxC\n",
        "    blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
        "\n",
        "    return np.float32(blended)\n",
        "\n",
        "\n",
        "\n",
        "img_c = image.load_img(content_url)\n",
        "img_c = image.img_to_array(img_c)\n",
        "img_c_shape = img_c.shape\n",
        "img_c = np.expand_dims(img_c, axis=0)\n",
        "\n",
        "img_s = image.load_img(style_url)\n",
        "img_s = image.img_to_array(img_s)\n",
        "img_s_shape = img_s.shape\n",
        "img_s = np.expand_dims(img_s, axis=0)\n",
        "\n",
        "assert img_c_shape == img_s_shape, \\\n",
        "    'Content and style image should be the same shape, %s != %s' \\\n",
        "    % (str(img_c_shape), str(img_s_shape))\n",
        "\n",
        "input_shape = img_c_shape\n",
        "\n",
        "print('Loading decoders...')\n",
        "decoders = {}\n",
        "decoders[5] = load_model('./decoder_5-6ep.h5')\n",
        "decoders[4] = load_model('./decoder_4-ep3.h5')\n",
        "print('Loading VGG...')\n",
        "vgg = VGG19(input_shape=input_shape, target_layer=5)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(np.clip(img_s[0]/ 255, 0, 1))\n",
        "plt.show()\n",
        "\n",
        "print('Styling...')\n",
        "for i in [5,4,5,4]:\n",
        "    feats_c = get_vgg_features(vgg, img_c, i)\n",
        "    feats_s = get_vgg_features(vgg, img_s, i)\n",
        "    feats_cs = wct(feats_c, feats_s)\n",
        "    img_c = decoders[i].predict(feats_cs)\n",
        "    plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
        "    plt.show()\n",
        "\n",
        "print('Saving output...')\n",
        "output_img = img_c[0]\n",
        "\n",
        "imageio.imwrite(OUTPUT_IMG_PATH, output_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM8IDM6PCpnl",
        "colab_type": "text"
      },
      "source": [
        "## Add smoothing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QxwsYHUzI3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.ndimage.filters import convolve\n",
        "def getSigmas(img_con,k):\n",
        "  conv_img = convolve(img_con, k, mode='constant', cval=0.0)\n",
        "  return (img_con - conv_img) **2\n",
        "k = np.array([[1/9,1/9,1/9],\n",
        "              [1/9,1/9,1/9],\n",
        "              [1/9,1/9,1/9]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCDsUzFhTTHa",
        "colab_type": "code",
        "outputId": "81c8ae12-b1cd-4867-af58-ef006df0840c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "import math\n",
        "stylized_url = './style-transfer-1.jpg'\n",
        "content_url = './download.jpg'\n",
        "img_stylized = image.load_img(stylized_url)\n",
        "img_stylized = image.img_to_array(img_stylized)\n",
        "img_stylized_shape = img_stylized.shape\n",
        "# img_stylized = np.expand_dims(img_stylized, axis=0)\n",
        "# getting sigmas\n",
        "img_con = image.load_img(content_url)\n",
        "img_con = image.img_to_array(img_con)\n",
        "img_con_shape = img_con.shape\n",
        "# img_con = np.expand_dims(img_con, axis=0)\n",
        "img_avg = []\n",
        "#gray scale\n",
        "for i in range(256):\n",
        "  img_avg.append([])\n",
        "  for j in range(256):\n",
        "    avg = (img_con[i][j][0] + img_con[i][j][1] + img_con[i][j][2]) / 3\n",
        "    img_avg[i].append(avg)\n",
        "img_con = np.array(img_avg)\n",
        "sigmas = getSigmas(img_con,k)\n",
        "#zero breaks things cause dividing by zero\n",
        "for row,val in enumerate(sigmas):\n",
        "  for col,val2 in enumerate(val):\n",
        "    if(val2 == 0):\n",
        "      sigmas[row][col] = 0.0001\n",
        "gg = coo_matrix(((256*256), (256*256)), dtype=np.float32)\n",
        "gg = gg.tocsr()\n",
        "print(gg)\n",
        "pixel_val = 0\n",
        "#building out Wij\n",
        "for row,val in enumerate(img_con):\n",
        "  for col,val2 in enumerate(val):\n",
        "    #edge case for the same row and col values so it is just zero\n",
        "    # for some reason getting rid of the continue fixed stuff and i am not sure why\n",
        "    if(row == col):\n",
        "      gg[pixel_val,pixel_val] = 0\n",
        "    #pixel to the right\n",
        "    if(col < 255):\n",
        "      gg[pixel_val,pixel_val+1] = math.exp(-((img_con[row][col] - img_con[row][col+1])**2)/sigmas[row][col])\n",
        "    #pixel to the left\n",
        "    if(col > 0):\n",
        "      gg[pixel_val,pixel_val-1] =  math.exp(-((img_con[row][col] - img_con[row][col-1])**2)/sigmas[row][col])\n",
        "    #pixel to the upper right\n",
        "    if(col < 255 and row > 0):\n",
        "      gg[pixel_val,pixel_val-255] =  math.exp(-((img_con[row][col] - img_con[row-1][col+1])**2)/sigmas[row][col])\n",
        "    #pixel above\n",
        "    if(row>0):\n",
        "      gg[pixel_val,pixel_val-256] =  math.exp(-((img_con[row][col] - img_con[row-1][col])**2)/sigmas[row][col])\n",
        "    #pixel to upper left\n",
        "    if(col > 0 and row >0):\n",
        "      gg[pixel_val,pixel_val-257] =  math.exp(-((img_con[row][col] - img_con[row-1][col-1])**2)/sigmas[row][col])\n",
        "    #pixel to lower right\n",
        "    if(row < 255 and col < 255 ):\n",
        "      gg[pixel_val,pixel_val+257] =  math.exp(-((img_con[row][col] - img_con[row+1][col+1])**2)/sigmas[row][col])\n",
        "    #pixel below\n",
        "    if(row < 255):\n",
        "      gg[pixel_val,pixel_val+256] =  math.exp(-((img_con[row][col] - img_con[row+1][col])**2)/sigmas[row][col])\n",
        "    #pixel to lower left\n",
        "    if(row < 255 and col > 0):\n",
        "      gg[pixel_val,pixel_val+255] =  math.exp(-((img_con[row][col] - img_con[row+1][col-1])**2)/sigmas[row][col])\n",
        "    pixel_val +=1\n",
        "  print(pixel_val)\n",
        "# # calc D\n",
        "\n",
        "# # calc W\n",
        "# R = (1 - alpha) * (identity_matrix - alpha*D)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "512\n",
            "768\n",
            "1024\n",
            "1280\n",
            "1536\n",
            "1792\n",
            "2048\n",
            "2304\n",
            "2560\n",
            "2816\n",
            "3072\n",
            "3328\n",
            "3584\n",
            "3840\n",
            "4096\n",
            "4352\n",
            "4608\n",
            "4864\n",
            "5120\n",
            "5376\n",
            "5632\n",
            "5888\n",
            "6144\n",
            "6400\n",
            "6656\n",
            "6912\n",
            "7168\n",
            "7424\n",
            "7680\n",
            "7936\n",
            "8192\n",
            "8448\n",
            "8704\n",
            "8960\n",
            "9216\n",
            "9472\n",
            "9728\n",
            "9984\n",
            "10240\n",
            "10496\n",
            "10752\n",
            "11008\n",
            "11264\n",
            "11520\n",
            "11776\n",
            "12032\n",
            "12288\n",
            "12544\n",
            "12800\n",
            "13056\n",
            "13312\n",
            "13568\n",
            "13824\n",
            "14080\n",
            "14336\n",
            "14592\n",
            "14848\n",
            "15104\n",
            "15360\n",
            "15616\n",
            "15872\n",
            "16128\n",
            "16384\n",
            "16640\n",
            "16896\n",
            "17152\n",
            "17408\n",
            "17664\n",
            "17920\n",
            "18176\n",
            "18432\n",
            "18688\n",
            "18944\n",
            "19200\n",
            "19456\n",
            "19712\n",
            "19968\n",
            "20224\n",
            "20480\n",
            "20736\n",
            "20992\n",
            "21248\n",
            "21504\n",
            "21760\n",
            "22016\n",
            "22272\n",
            "22528\n",
            "22784\n",
            "23040\n",
            "23296\n",
            "23552\n",
            "23808\n",
            "24064\n",
            "24320\n",
            "24576\n",
            "24832\n",
            "25088\n",
            "25344\n",
            "25600\n",
            "25856\n",
            "26112\n",
            "26368\n",
            "26624\n",
            "26880\n",
            "27136\n",
            "27392\n",
            "27648\n",
            "27904\n",
            "28160\n",
            "28416\n",
            "28672\n",
            "28928\n",
            "29184\n",
            "29440\n",
            "29696\n",
            "29952\n",
            "30208\n",
            "30464\n",
            "30720\n",
            "30976\n",
            "31232\n",
            "31488\n",
            "31744\n",
            "32000\n",
            "32256\n",
            "32512\n",
            "32768\n",
            "33024\n",
            "33280\n",
            "33536\n",
            "33792\n",
            "34048\n",
            "34304\n",
            "34560\n",
            "34816\n",
            "35072\n",
            "35328\n",
            "35584\n",
            "35840\n",
            "36096\n",
            "36352\n",
            "36608\n",
            "36864\n",
            "37120\n",
            "37376\n",
            "37632\n",
            "37888\n",
            "38144\n",
            "38400\n",
            "38656\n",
            "38912\n",
            "39168\n",
            "39424\n",
            "39680\n",
            "39936\n",
            "40192\n",
            "40448\n",
            "40704\n",
            "40960\n",
            "41216\n",
            "41472\n",
            "41728\n",
            "41984\n",
            "42240\n",
            "42496\n",
            "42752\n",
            "43008\n",
            "43264\n",
            "43520\n",
            "43776\n",
            "44032\n",
            "44288\n",
            "44544\n",
            "44800\n",
            "45056\n",
            "45312\n",
            "45568\n",
            "45824\n",
            "46080\n",
            "46336\n",
            "46592\n",
            "46848\n",
            "47104\n",
            "47360\n",
            "47616\n",
            "47872\n",
            "48128\n",
            "48384\n",
            "48640\n",
            "48896\n",
            "49152\n",
            "49408\n",
            "49664\n",
            "49920\n",
            "50176\n",
            "50432\n",
            "50688\n",
            "50944\n",
            "51200\n",
            "51456\n",
            "51712\n",
            "51968\n",
            "52224\n",
            "52480\n",
            "52736\n",
            "52992\n",
            "53248\n",
            "53504\n",
            "53760\n",
            "54016\n",
            "54272\n",
            "54528\n",
            "54784\n",
            "55040\n",
            "55296\n",
            "55552\n",
            "55808\n",
            "56064\n",
            "56320\n",
            "56576\n",
            "56832\n",
            "57088\n",
            "57344\n",
            "57600\n",
            "57856\n",
            "58112\n",
            "58368\n",
            "58624\n",
            "58880\n",
            "59136\n",
            "59392\n",
            "59648\n",
            "59904\n",
            "60160\n",
            "60416\n",
            "60672\n",
            "60928\n",
            "61184\n",
            "61440\n",
            "61696\n",
            "61952\n",
            "62208\n",
            "62464\n",
            "62720\n",
            "62976\n",
            "63232\n",
            "63488\n",
            "63744\n",
            "64000\n",
            "64256\n",
            "64512\n",
            "64768\n",
            "65024\n",
            "65280\n",
            "65536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgoswRjyaM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6a560598-8b82-4b39-f24b-0e316b881a8e"
      },
      "source": [
        "d = coo_matrix(((256*256), (256*256)), dtype=np.float32)\n",
        "d = d.tocsr()\n",
        "for row,val in enumerate(gg):\n",
        "  d_val = 0\n",
        "  #pixel to the right\n",
        "  if(row % 256  != 255):\n",
        "    d_val += gg[row,row+1] \n",
        "  #pixel to the left\n",
        "  if(row % 256  != 0):\n",
        "    d_val += gg[row,row-1] \n",
        "  #pixel to the upper right\n",
        "  if((row % 256  != 255) and row > 255):\n",
        "    d_val += gg[row,row-255]\n",
        "  #pixel above\n",
        "  if(row>255):\n",
        "    d_val += gg[row,row-256]\n",
        "  #pixel to upper left\n",
        "  if((row % 256  != 0) and row >255):\n",
        "    d_val += gg[row,row-257] \n",
        "  #pixel to lower right\n",
        "  if(row < 65280 and (row % 256  != 255)):\n",
        "    d_val += gg[row,row+257]\n",
        "  #pixel below\n",
        "  if(row < 65280):\n",
        "    d_val += gg[row,row+256] \n",
        "  #pixel to lower left\n",
        "  if(row < 65280 and (row % 256  != 0)):\n",
        "    d_val += gg[row,row+255]\n",
        "  if(d_val == 0):\n",
        "    d_val = 0.001\n",
        "  d[row,row] = (1/d_val)**(1/2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRPjDTxRFF_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8698c16e-c8f7-4679-a93a-cdcc472fe8ed"
      },
      "source": [
        "y = coo_matrix(((256*256), (256*256)), dtype=np.float32)\n",
        "y = y.tocsr()\n",
        "#building out Y\n",
        "pixel_val = 0\n",
        "for row,val in enumerate(img_con):\n",
        "  for col,val2 in enumerate(val):\n",
        "    y[pixel_val,pixel_val] = val2\n",
        "    pixel_val+=1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVX8ktSNF2nA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a2783106-aa6e-47be-8e9e-d10a7703e785"
      },
      "source": [
        "i = coo_matrix(((256*256), (256*256)), dtype=np.float32)\n",
        "i = i.tocsr()\n",
        "#building out i\n",
        "pixel_val = 0\n",
        "for row,val in enumerate(img_con):\n",
        "  for col,val2 in enumerate(val):\n",
        "    i[pixel_val,pixel_val] = 1\n",
        "    pixel_val+=1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxvoJanfwsk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# warning said these would be fast\n",
        "gg = gg.tocsc()\n",
        "d = d.tocsc()\n",
        "i = i.tocsc()\n",
        "y = y.tocsc()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdxAdqQQFoSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6649effa-acfb-4c82-b958-f68b83288493"
      },
      "source": [
        "from scipy.sparse.linalg import inv\n",
        "alpha = 0.5\n",
        "part1 =  (d @ gg @ d)\n",
        "print('part1')\n",
        "part2 = i - (alpha * part1)\n",
        "print('part2')\n",
        "part3 = inv(part2) \n",
        "print('part3')\n",
        "part4 = (1.0-alpha) *  part3\n",
        "print('part4')\n",
        "R = part4 @ y\n",
        "print(':)')\n",
        "# R = (1.0-alpha) * (i - alpha * d @ gg @ d ) ** (-1.0) @ y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part1\n",
            "part2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-YGdM0MKgyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(R.shape)\n",
        "print(R[0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1WrvmZPYVSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = np.zeros((1,256, 256,3))\n",
        "#building out Y\n",
        "pixel_val = 0\n",
        "for row,val in enumerate(img_con):\n",
        "  for col,val2 in enumerate(val):\n",
        "    result[0][row][col] = [R[pixel_val,pixel_val],R[pixel_val,pixel_val],R[pixel_val,pixel_val]]\n",
        "    pixel_val+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33hmdoPmZEdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(img_c.shape)\n",
        "print(result.shape)\n",
        "plt.imshow(np.clip(result[0] / 255, 0, 1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlFnw9ELCpnm",
        "colab_type": "code",
        "outputId": "41931858-c1db-405c-dc02-9eb4c7aacc8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install keras --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0jfoh2nC7S7",
        "colab_type": "code",
        "outputId": "ccf70140-ca09-4ccc-ded8-adea94ef8f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/val2014.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 17:28:05--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.227.128\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.227.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: val2014.zip\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  16.9MB/s    in 2m 21s  \n",
            "\n",
            "2020-03-04 17:30:27 (44.8 MB/s) - val2014.zip saved [6645013297/6645013297]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUeBydLPiasB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip val2014.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJBsEk2GrAGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-19TOgRhP1SX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "\n",
        "\n",
        "class MaxPoolingWithArgmax2D(Layer):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            pool_size=(2, 2),\n",
        "            strides=(2, 2),\n",
        "            padding='same',\n",
        "            **kwargs):\n",
        "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
        "        self.padding = padding\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        padding = self.padding\n",
        "        pool_size = self.pool_size\n",
        "        strides = self.strides\n",
        "        if K.backend() == 'tensorflow':\n",
        "            ksize = [1, pool_size[0], pool_size[1], 1]\n",
        "            padding = padding.upper()\n",
        "            strides = [1, strides[0], strides[1], 1]\n",
        "            output, argmax = tf.nn.max_pool_with_argmax(\n",
        "                    inputs,\n",
        "                    ksize=ksize,\n",
        "                    strides=strides,\n",
        "                    padding=padding)\n",
        "        else:\n",
        "            errmsg = '{} backend is not supported for layer {}'.format(\n",
        "                    K.backend(), type(self).__name__)\n",
        "            raise NotImplementedError(errmsg)\n",
        "        argmax = K.cast(argmax, K.floatx())\n",
        "        return [output, argmax]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ratio = (1, 2, 2, 1)\n",
        "        output_shape = [\n",
        "                dim//ratio[idx]\n",
        "                if dim is not None else None\n",
        "                for idx, dim in enumerate(input_shape)]\n",
        "        output_shape = tuple(output_shape)\n",
        "        return [output_shape, output_shape]\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return 2 * [None]\n",
        "\n",
        "\n",
        "class MaxUnpooling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), **kwargs):\n",
        "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, inputs, output_shape=None):\n",
        "        updates, mask = inputs[0], inputs[1]\n",
        "        with K.tf.variable_scope(self.name):\n",
        "            mask = K.cast(mask, 'int32')\n",
        "            input_shape = K.tf.shape(updates, out_type='int32')\n",
        "            #  calculation new shape\n",
        "            if output_shape is None:\n",
        "                output_shape = (\n",
        "                        input_shape[0],\n",
        "                        input_shape[1]*self.size[0],\n",
        "                        input_shape[2]*self.size[1],\n",
        "                        input_shape[3])\n",
        "            self.output_shape1 = output_shape\n",
        "\n",
        "            # calculation indices for batch, height, width and feature maps\n",
        "            one_like_mask = K.ones_like(mask, dtype='int32')\n",
        "            batch_shape = K.concatenate(\n",
        "                    [[input_shape[0]], [1], [1], [1]],\n",
        "                    axis=0)\n",
        "            batch_range = K.reshape(\n",
        "                    K.tf.range(output_shape[0], dtype='int32'),\n",
        "                    shape=batch_shape)\n",
        "            b = one_like_mask * batch_range\n",
        "            y = mask // (output_shape[2] * output_shape[3])\n",
        "            x = (mask // output_shape[3]) % output_shape[2]\n",
        "            feature_range = K.tf.range(output_shape[3], dtype='int32')\n",
        "            f = one_like_mask * feature_range\n",
        "\n",
        "            # transpose indices & reshape update values to one dimension\n",
        "            updates_size = K.tf.size(updates)\n",
        "            indices = K.transpose(K.reshape(\n",
        "                K.stack([b, y, x, f]),\n",
        "                [4, updates_size]))\n",
        "            values = K.reshape(updates, [updates_size])\n",
        "            ret = K.tf.scatter_nd(indices, values, output_shape)\n",
        "            return ret\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        mask_shape = input_shape[1]\n",
        "        return (\n",
        "                mask_shape[0],\n",
        "                mask_shape[1]*self.size[0],\n",
        "                mask_shape[2]*self.size[1],\n",
        "                mask_shape[3]\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtmQ-sy_X6i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.utils import conv_utils\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, Lambda, Conv2D\n",
        "# from tensorflow.python.ops import nn_ops\n",
        "from keras import initializers, regularizers, constraints, activations\n",
        "from keras.utils import conv_utils\n",
        "class MaxPoolingWithArgmax2D(Layer):\n",
        "    '''MaxPooling for unpooling with indices.\n",
        "    \n",
        "    # References\n",
        "        [SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](http://arxiv.org/abs/1511.00561)\n",
        "    \n",
        "    # related code:\n",
        "        https://github.com/PavlosMelissinos/enet-keras\n",
        "        https://github.com/ykamikawa/SegNet\n",
        "    '''\n",
        "    def __init__(self, pool_size=(2, 2), strides=(2, 2), padding='same', **kwargs):\n",
        "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
        "        self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        ksize = [1, self.pool_size[0], self.pool_size[1], 1]\n",
        "        strides = [1, self.strides[0], self.strides[1], 1]\n",
        "        padding = self.padding.upper()\n",
        "        output, argmax = tf.nn.max_pool_with_argmax(inputs, ksize, strides, padding)\n",
        "        argmax = tf.cast(argmax, K.floatx())\n",
        "        return [output, argmax]\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ratio = (1, 2, 2, 1)\n",
        "        output_shape = [dim // ratio[idx] if dim is not None else None for idx, dim in enumerate(input_shape)]\n",
        "        output_shape = tuple(output_shape)\n",
        "        return [output_shape, output_shape]\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return 2 * [None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbAIl3QfxOpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaxUnpooling2D(Layer):\n",
        "    '''Inversion of MaxPooling with indices.\n",
        "    \n",
        "    # References\n",
        "        [SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](http://arxiv.org/abs/1511.00561)\n",
        "    \n",
        "    # related code:\n",
        "        https://github.com/PavlosMelissinos/enet-keras\n",
        "        https://github.com/ykamikawa/SegNet\n",
        "    '''\n",
        "    def __init__(self, size=(2, 2), **kwargs):\n",
        "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
        "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n",
        "\n",
        "    def call(self, inputs, output_shape=None):\n",
        "        updates, mask = inputs[0], inputs[1]\n",
        "        \n",
        "        mask = tf.cast(mask, 'int32')\n",
        "        input_shape = tf.shape(updates, out_type='int32')\n",
        "        #  calculation new shape\n",
        "        if output_shape is None:\n",
        "            output_shape = (input_shape[0], input_shape[1] * self.size[0], input_shape[2] * self.size[1], input_shape[3])\n",
        "        \n",
        "        # calculation indices for batch, height, width and feature maps\n",
        "        one_like_mask = K.ones_like(mask, dtype='int32')\n",
        "        batch_shape = K.concatenate([[input_shape[0]], [1], [1], [1]], axis=0)\n",
        "        batch_range = K.reshape(tf.range(output_shape[0], dtype='int32'), shape=batch_shape)\n",
        "        b = one_like_mask * batch_range\n",
        "        y = mask // (output_shape[2] * output_shape[3])\n",
        "        x = (mask // output_shape[3]) % output_shape[2]\n",
        "        feature_range = tf.range(output_shape[3], dtype='int32')\n",
        "        f = one_like_mask * feature_range\n",
        "        \n",
        "        # transpose indices & reshape update values to one dimension\n",
        "        updates_size = tf.size(updates)\n",
        "        indices = K.transpose(K.reshape(K.stack([b, y, x, f]), [4, updates_size]))\n",
        "        values = K.reshape(updates, [updates_size])\n",
        "        ret = tf.scatter_nd(indices, values, output_shape)\n",
        "        return ret\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        mask_shape = input_shape[1]\n",
        "        output_shape = [mask_shape[0], mask_shape[1] * self.size[0], mask_shape[2] * self.size[1], mask_shape[3]]\n",
        "        return tuple(output_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3EI8-veCpnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.layers import Concatenate\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
        "\n",
        "# WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "#                         WEIGHTS_PATH_NO_TOP,\n",
        "#                         cache_subdir='models',\n",
        "#                         file_hash='253f8cb515780f3b799900260a226db6')\n",
        "\n",
        "def vgg_layers(inputs, target_layer):\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
        "    if target_layer == 1:\n",
        "        return x\n",
        "    #added this to make stirdes to the convolution layer\n",
        "    #honestly not sure if this is what he wanted\n",
        "    #repeated it for each ending conolution in VGG\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x, arg_1 = MaxPoolingWithArgmax2D(pool_size=2,strides=(2,2),name='block1_pool')(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "    print(x)\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    if target_layer == 2:\n",
        "        return x\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x, arg_2 = MaxPoolingWithArgmax2D(pool_size=2,strides=(2,2),name='block2_pool')(x)\n",
        "\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    if target_layer == 3:\n",
        "        return x\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
        "    x, arg_3 = MaxPoolingWithArgmax2D(pool_size=2,strides=(2,2),name='block3_pool')(x)\n",
        "\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "    # argmax3 = K.cast(argmax3, K.floatx())\n",
        "    # print(argmax3)\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    if target_layer == 4:\n",
        "        return x\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
        "    # x, argmax4 = tf.nn.max_pool_with_argmax(\n",
        "    #                 x,\n",
        "    #                 ksize=(2,2),\n",
        "    #                 strides=(2,2),\n",
        "    #                 padding='SAME', name= 'block4_pool')\n",
        "    # argmax4 = K.cast(argmax4, K.floatx())\n",
        "    # print(argmax4)\n",
        "    x, arg_4 = MaxPoolingWithArgmax2D(pool_size=2,strides=(2,2),name='block4_pool')(x)\n",
        "\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    return Concatenate([x,arg_1,arg_2,arg_3,arg_4])\n",
        "\n",
        "\n",
        "def load_weights(model):\n",
        "    f = h5py.File(WEIGHTS_PATH)\n",
        "    layer_names = [name for name in f.attrs['layer_names']]\n",
        "\n",
        "    for layer in model.layers:\n",
        "        b_name = layer.name.encode()\n",
        "        if b_name in layer_names:\n",
        "            g = f[b_name]\n",
        "            weights = [g[name] for name in g.attrs['weight_names']]\n",
        "            layer.set_weights(weights)\n",
        "            layer.trainable = False\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
        "    \"\"\"\n",
        "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
        "    \"\"\"\n",
        "    if input_tensor is None:\n",
        "        inputs = Input(shape=input_shape)\n",
        "    else:\n",
        "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
        "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19')\n",
        "    load_weights(model)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_input(x):\n",
        "    # Convert 'RGB' -> 'BGR'\n",
        "    if type(x) is np.ndarray:\n",
        "        x = x[..., ::-1]\n",
        "    else:\n",
        "        x = tf.reverse(x, [-1])\n",
        "\n",
        "    return x - MEAN_PIXEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gngxuaqJCpnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Conv2D, Input\n",
        "LAMBDA=1\n",
        "\n",
        "def l2_loss(x):\n",
        "    return K.sum(K.square(x)) / 2\n",
        "\n",
        "class EncoderDecoder:\n",
        "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
        "                 decoder_path=None):\n",
        "        self.input_shape = input_shape\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.encoder = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
        "        if decoder_path:\n",
        "            self.decoder = load_model(decoder_path)\n",
        "        else:\n",
        "            self.decoder = self.create_decoder(target_layer)\n",
        "\n",
        "        self.model = \n",
        "        self.model.add(self.encoder)\n",
        "        self.model.add(self.decoder)\n",
        "\n",
        "        self.loss = self.create_loss_fn(self.encoder)\n",
        "\n",
        "        self.model.compile('adam', self.loss)\n",
        "\n",
        "    def create_loss_fn(self, encoder):\n",
        "        def get_encodings(inputs):\n",
        "            encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
        "            return encoder.output\n",
        "\n",
        "        def loss(img_in, img_out):\n",
        "            encoding_in = get_encodings(img_in)\n",
        "            encoding_out = get_encodings(img_out)\n",
        "            return l2_loss(img_out - img_in) + \\\n",
        "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
        "        return loss\n",
        "\n",
        "    def create_decoder(self, target_layer):\n",
        "        print('output shape',self.encoder.output_shape[0][1:])\n",
        "        inputs = Input(shape=self.encoder.output_shape[0][1:])\n",
        "        layers = decoder_layers(inputs, target_layer)\n",
        "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
        "                        name='decoder_out')(layers)\n",
        "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
        "\n",
        "    def export_decoder(self):\n",
        "        self.decoder.save('decoder_%s.h5' % self.target_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsqErcCUCpnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Conv2D, UpSampling2D, Conv2DTranspose\n",
        "\n",
        "def decoder_layers(inputs,layer):\n",
        "    print(inputs)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
        "    if layer == 1:\n",
        "        return x\n",
        "    print('xxxx',x)\n",
        "    # x = MaxUnpooling2D(size=2,name='decoder_unpool1')([x,indices[0][0]])\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
        "    if layer == 2:\n",
        "        return x\n",
        "\n",
        "    # x = MaxUnpooling2D(size=2,name='decoder_unpool2')([x,indices[1][0]])\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
        "    if layer == 3:\n",
        "        return x\n",
        "\n",
        "    # x = MaxUnpooling2D(size=2,name='decoder_unpool3')([x,indices[2][0]])\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
        "    if layer == 4:\n",
        "        return x\n",
        "\n",
        "    # x = MaxUnpooling2D(size=2,name='decoder_unpool4')([x,indices[3][0]])\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
        "    if layer == 5:\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}