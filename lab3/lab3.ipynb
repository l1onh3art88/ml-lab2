{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Our task for this lab is to create our own logistic regression model which is able to classify how many Uber pickups there will be (low, medium, or high) based off of different information in our dataset. The dataset is a collection of information about Uber pickups like time and location, joined with other data such as the weather for that time and location, what borough it is in, and whether or not it was a NYC public holiday. We split our predictions up by borough because certain boroughs like Manhattan generally always have a higher volume of pickups than boroughs like the Bronx, so aggregate predictions over all of NYC would not have been very insightful. Instead, we make predictions specific to each borough, with the exception of EWR and Staten Island, which we threw out because they did not contain enough data to make accurate predictions. We denote a \"high\" amount of pickups as greater than half a standard deviation above the mean for that borough. A \"low\" amount is less than half a standard deviation below the mean for that borough. A \"medium\" amount is inbetween. \n",
    "\n",
    "Our predicion task is valuable because it gives Uber insight into the time periods where they can be most profitable, and time periods where they can save money. For example, on New Years Eve there is most likely an extreme surge in the number of rides requested. If there are not enough drivers to satisfy all of these rides, people will go to Lyft or even just hail a yellow cab. However, if they prepare for this surge by incentivising drivers with an extra percentage of the ride money, there will be more drivers to satisfy the extra rides requests. Our model's insights would help pull in more profits and increases market share compared to treating every day and location as equally profitable. In production, this model would provide the best results if it were deployed so that it would run constantly and react to changing weather conditions, social movements, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/uber_nyc_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>490.215903</td>\n",
       "      <td>5.984924</td>\n",
       "      <td>8.818125</td>\n",
       "      <td>47.669042</td>\n",
       "      <td>30.823065</td>\n",
       "      <td>1017.817938</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.090464</td>\n",
       "      <td>2.529169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>995.649536</td>\n",
       "      <td>3.699007</td>\n",
       "      <td>2.442897</td>\n",
       "      <td>19.814969</td>\n",
       "      <td>21.283444</td>\n",
       "      <td>7.768796</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.093125</td>\n",
       "      <td>0.219402</td>\n",
       "      <td>4.520325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>991.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1012.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>449.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1022.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>2.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7883.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1043.400000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickups           spd           vsb          temp          dewp  \\\n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000   \n",
       "mean     490.215903      5.984924      8.818125     47.669042     30.823065   \n",
       "std      995.649536      3.699007      2.442897     19.814969     21.283444   \n",
       "min        0.000000      0.000000      0.000000      2.000000    -16.000000   \n",
       "25%        1.000000      3.000000      9.100000     32.000000     14.000000   \n",
       "50%       54.000000      6.000000     10.000000     46.000000     30.000000   \n",
       "75%      449.000000      8.000000     10.000000     64.500000     50.000000   \n",
       "max     7883.000000     21.000000     10.000000     89.000000     73.000000   \n",
       "\n",
       "                slp         pcp01         pcp06         pcp24            sd  \n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000  \n",
       "mean    1017.817938      0.003830      0.026129      0.090464      2.529169  \n",
       "std        7.768796      0.018933      0.093125      0.219402      4.520325  \n",
       "min      991.400000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%     1012.500000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%     1018.200000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%     1022.900000      0.000000      0.000000      0.050000      2.958333  \n",
       "max     1043.400000      0.280000      1.240000      2.100000     19.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> checking for nan or null values in the dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found only Borough has nan values so we remove the nan rows </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough       True\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough      False\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that most of our data didnt have much correlation except temperate and the dew point temperature. We decided to get rid of this variable becasue it seemed very similar to temperature and did not think it would impact the machine learning. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pickups</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.015708</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.002821</td>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.009676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spd</th>\n",
       "      <td>0.009741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.097041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vsb</th>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.047834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.063692</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>-0.545558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dewp</th>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.489372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slp</th>\n",
       "      <td>-0.015708</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.121508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp01</th>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp06</th>\n",
       "      <td>-0.002821</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.039943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp24</th>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>-0.009676</td>\n",
       "      <td>0.097041</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.545558</td>\n",
       "      <td>-0.489372</td>\n",
       "      <td>0.121508</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>0.069664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickups       spd       vsb      temp      dewp       slp     pcp01  \\\n",
       "pickups  1.000000  0.009741 -0.008429  0.063692  0.040082 -0.015708  0.005007   \n",
       "spd      0.009741  1.000000  0.086177 -0.296126 -0.321606 -0.092761 -0.000357   \n",
       "vsb     -0.008429  0.086177  1.000000  0.025214 -0.231294  0.167039 -0.488407   \n",
       "temp     0.063692 -0.296126  0.025214  1.000000  0.896544 -0.224537 -0.013343   \n",
       "dewp     0.040082 -0.321606 -0.231294  0.896544  1.000000 -0.311156  0.115399   \n",
       "slp     -0.015708 -0.092761  0.167039 -0.224537 -0.311156  1.000000 -0.089752   \n",
       "pcp01    0.005007 -0.000357 -0.488407 -0.013343  0.115399 -0.089752  1.000000   \n",
       "pcp06   -0.002821  0.016668 -0.118346 -0.037295  0.013293 -0.104940  0.128064   \n",
       "pcp24   -0.022935 -0.010412  0.000895 -0.014408  0.001519 -0.134689  0.000997   \n",
       "sd      -0.009676  0.097041 -0.047834 -0.545558 -0.489372  0.121508  0.000310   \n",
       "\n",
       "            pcp06     pcp24        sd  \n",
       "pickups -0.002821 -0.022935 -0.009676  \n",
       "spd      0.016668 -0.010412  0.097041  \n",
       "vsb     -0.118346  0.000895 -0.047834  \n",
       "temp    -0.037295 -0.014408 -0.545558  \n",
       "dewp     0.013293  0.001519 -0.489372  \n",
       "slp     -0.104940 -0.134689  0.121508  \n",
       "pcp01    0.128064  0.000997  0.000310  \n",
       "pcp06    1.000000  0.251166  0.039943  \n",
       "pcp24    0.251166  1.000000  0.069664  \n",
       "sd       0.039943  0.069664  1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['dewp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We made the holiday column count 1 for yes and 0 for no. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hday'] = data['hday'].apply(lambda x: 0 if x=='N' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We one hot encoded our boroughs becuase they were string values </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotCols = pd.get_dummies(data['borough'])\n",
    "data = data.join(oneHotCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['borough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the time of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We based our hour groups by sunrise and sunset. Night is the time when the sun is down, which on average is from 8pm to 6am. Morning is from 6am till noon. Afternoon is from noon till 5pm. Evening is from 5pm till 8pm, which is around when the sunsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dateTest = data['pickup_dt'][0]\n",
    "print(int(dateTest[11:13]))\n",
    "data['is_morning'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 6 and int(x[11:13]) < 12) else 0)\n",
    "data['is_afternoon'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 12 and int(x[11:13]) < 17) else 0)\n",
    "data['is_evening'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 17 and int(x[11:13]) < 21) else 0)\n",
    "data['is_night']  = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 21 or int(x[11:13]) < 6) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the weekday\n",
    "\n",
    "The weekday from the pickup_dt feature has been 1 hot encoded into monday-sunday. We believe having each day as a feature will help classify & predict the number of ubers necessary at a future date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "data['is_monday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 0 else 0)\n",
    "data['is_tuesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 1 else 0)\n",
    "data['is_wednesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 2 else 0)\n",
    "data['is_thursday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 3 else 0)\n",
    "data['is_friday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 4 else 0)\n",
    "data['is_saturday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 5 else 0)\n",
    "data['is_sunday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['pickup_dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that the borough EWR averages about 2.4 pickups every 96 hours so we are getting rid of the EWR borough from our dataset. We found that the borough Staten Island averages 1.6 pickups and hour and had a max 13 pickups in an hour over 6 months so we got rid of it from our dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           pickups     EWR\n",
      "count  4343.000000  4343.0\n",
      "mean      0.024177     1.0\n",
      "std       0.160937     0.0\n",
      "min       0.000000     1.0\n",
      "25%       0.000000     1.0\n",
      "50%       0.000000     1.0\n",
      "75%       0.000000     1.0\n",
      "max       2.000000     1.0\n",
      "           pickups  Staten Island\n",
      "count  4343.000000         4343.0\n",
      "mean      1.601888            1.0\n",
      "std       1.640451            0.0\n",
      "min       0.000000            1.0\n",
      "25%       0.000000            1.0\n",
      "50%       1.000000            1.0\n",
      "75%       2.000000            1.0\n",
      "max      13.000000            1.0\n"
     ]
    }
   ],
   "source": [
    "d1 = data.where(data['EWR']==1)[['pickups','EWR']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data.EWR == 0]\n",
    "del data['EWR']\n",
    "d1 = data.where(data['Staten Island']==1)[['pickups','Staten Island']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data['Staten Island'] == 0]\n",
    "del data['Staten Island']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We have three cateogries of pickup traffic low, medium, high. We found these by finding the mean and standard deviation of each borough. Low is half a standard deviation below the mean and high is half a stadard deviation above the mean. Anything else is counted as a medium amount of pickups. This means that the category of low, medium, and high pickup amount depends on the borough. If we did base our categories by borough then Manhattan would always be in the high pickup amount category and Queens would always be in the low pickup amount category.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4343.000000\n",
      "mean     2387.253281\n",
      "std      1434.724668\n",
      "min         0.000000\n",
      "25%      1223.500000\n",
      "50%      2269.000000\n",
      "75%      3293.500000\n",
      "max      7883.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean       50.667050\n",
      "std        31.029223\n",
      "min         0.000000\n",
      "25%        29.000000\n",
      "50%        46.000000\n",
      "75%        66.000000\n",
      "max       262.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      309.354824\n",
      "std       154.368300\n",
      "min         0.000000\n",
      "25%       196.000000\n",
      "50%       308.000000\n",
      "75%       410.000000\n",
      "max       831.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      534.431269\n",
      "std       294.810182\n",
      "min         0.000000\n",
      "25%       331.500000\n",
      "50%       493.000000\n",
      "75%       675.000000\n",
      "max      2009.000000\n",
      "Name: pickups, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Man = data[data.Manhattan == 1]\n",
    "Bronx = data[data.Bronx == 1]\n",
    "Queens = data[data.Queens == 1]\n",
    "Brooklyn = data[data.Brooklyn == 1]\n",
    "\n",
    "print(Man['pickups'].describe())\n",
    "print(Bronx['pickups'].describe())\n",
    "print(Queens['pickups'].describe())\n",
    "print(Brooklyn['pickups'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Using the mean and stadard deviation of each borough to place each row of data into a category. We can see that our categories are almost completely balanced within each borough.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4343.000000\n",
      "mean        0.953028\n",
      "std         0.774480\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max         2.000000\n",
      "Name: pickupPrediction, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4343.000000\n",
      "mean        0.907898\n",
      "std         0.757453\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         2.000000\n",
      "Name: pickupPrediction, dtype: float64\n",
      "count    4343.000000\n",
      "mean        0.980428\n",
      "std         0.794867\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max         2.000000\n",
      "Name: pickupPrediction, dtype: float64\n",
      "count    4343.000000\n",
      "mean        0.901911\n",
      "std         0.757005\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         2.000000\n",
      "Name: pickupPrediction, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "mstd = Man['pickups'].std()\n",
    "mmean = Man['pickups'].mean()\n",
    "Man['pickupPrediction'] = Man['pickups'].apply(lambda x: 0 if x < (mmean - mstd/2) else (\n",
    "                                               2 if (x > (mmean + mstd/2))\n",
    "                                                else 1))\n",
    "print(Man['pickupPrediction'].describe())\n",
    "\n",
    "mstd = Bronx['pickups'].std()\n",
    "mmean = Bronx['pickups'].mean()\n",
    "Bronx['pickupPrediction'] = Bronx['pickups'].apply(lambda x: 0 if x < (mmean - mstd/2) else (\n",
    "                                               2 if (x > (mmean + mstd/2))\n",
    "                                                else 1))\n",
    "print(Bronx['pickupPrediction'].describe())\n",
    "\n",
    "mstd = Queens['pickups'].std()\n",
    "mmean = Queens['pickups'].mean()\n",
    "Queens['pickupPrediction'] = Queens['pickups'].apply(lambda x: 0 if x < (mmean - mstd/2) else (\n",
    "                                               2 if (x > (mmean + mstd/2))\n",
    "                                                else 1))\n",
    "print(Queens['pickupPrediction'].describe())\n",
    "\n",
    "mstd = Brooklyn['pickups'].std()\n",
    "mmean = Brooklyn['pickups'].mean()\n",
    "Brooklyn['pickupPrediction'] = Brooklyn['pickups'].apply(lambda x: 0 if x < (mmean - mstd/2) else (\n",
    "                                               2 if (x > (mmean + mstd/2))\n",
    "                                                else 1))\n",
    "print(Brooklyn['pickupPrediction'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "yMan = Man['pickupPrediction']\n",
    "del Man['pickupPrediction']\n",
    "yBronx = Bronx['pickupPrediction']\n",
    "del Bronx['pickupPrediction']\n",
    "yQueens = Queens['pickupPrediction']\n",
    "del Queens['pickupPrediction']\n",
    "yBrooklyn = Brooklyn['pickupPrediction']\n",
    "del Brooklyn['pickupPrediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Splitting our data in each borough before we combine our data again. This lets us have an even amount of each borough in the train and test dataset.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xManTrain, xManTest, yManTrain, YManTest = train_test_split(Man, yMan, test_size = 1/5, random_state = 0)\n",
    "xBronxTrain, xBronxTest, yBronxTrain, YBronxTest = train_test_split(Bronx, yBronx, test_size = 1/5, random_state = 0)\n",
    "xQueensTrain, xQueensTest, yQueensTrain, YQueensTest = train_test_split(Queens, yQueens, test_size = 1/5, random_state = 0)\n",
    "xBrooklynTrain, xBrooklynTest, yBrooklynTrain, YBrooklynTest = train_test_split(Brooklyn, yBrooklyn, test_size = 1/5, random_state = 0)\n",
    "xTrain = pd.concat([xManTrain,xBronxTrain,xQueensTrain,xBrooklynTrain])\n",
    "xTest = pd.concat([xManTest,xBronxTest,xQueensTest,xBrooklynTest])\n",
    "yTrain = pd.concat([yManTrain,yBronxTrain,yQueensTrain,yBrooklynTrain])\n",
    "yTest = pd.concat([YManTest,YBronxTest,YQueensTest,YBrooklynTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Putting our values of the pandas dataframe into arrays so that the index isn't taken as a feature during our machine learning. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "xTrain.reset_index(inplace=True, drop=True)\n",
    "yTrain.reset_index(inplace=True, drop=True)\n",
    "xTest.reset_index(inplace=True, drop=True)\n",
    "yTest.reset_index(inplace=True, drop=True)\n",
    "xTrain = xTrain.values\n",
    "yTrain = yTrain.values\n",
    "xTest = xTest.values\n",
    "yTest = yTest.values\n",
    "xTrain = StandardScaler().fit(xTrain).transform(xTrain)\n",
    "XTest = StandardScaler().fit(xTest).transform(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our modified version of the Binaray Logistic Regression from class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, L1C=0.001,L2C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.L1C = L1C\n",
    "        self.L2C = L2C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        print(y.shape)\n",
    "        print(ydiff.shape)\n",
    "        print(X.shape)\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        print('Gradient 1')\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.L1C > 0 and self.L2C > 0):\n",
    "            gradient[1:] += -2 * (np.sign(self.w_[1:]) * self.L1C + self.w_[1:] * self.L2C)\n",
    "        elif(self.L1C > 0):\n",
    "            gradient[1:] += -2 * np.sign(self.w_[1:]) * self.L1C \n",
    "        elif(self.L2C > 0):\n",
    "            gradient[1:] += (-2 * self.w_[1:] * self.L2C)\n",
    "        else:\n",
    "            gradient[1:] += -2 * self.w_[1:] \n",
    "        print('Gradient 2')\n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            print('here')\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our modified version of the Stochastic Logistic Regression from class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.L1C > 0 and self.L2C > 0):\n",
    "            gradient[1:] += -2 * (np.sign(self.w_[1:]) * self.L1C + self.w_[1:] * self.L2C)\n",
    "        elif(self.L1C > 0):\n",
    "            gradient[1:] += -2 * np.sign(self.w_[1:]) * self.L1C \n",
    "        elif(self.L2C > 0):\n",
    "            gradient[1:] += (-2 * self.w_[1:] * self.L2C)\n",
    "        else:\n",
    "            gradient[1:] += -2 * self.w_[1:] \n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our modified version of the Line Search Logistic Regression from class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,L1C,L2C):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        if(L1C > 0 and L2C > 0):\n",
    "            wobj = -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + L1C*sum(abs(wnew)) + L2C*sum(wnew**2)\n",
    "        elif(L1C > 0):\n",
    "            wobj = -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + L1C*sum(abs(wnew))\n",
    "        else:\n",
    "            wobj = -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + L2C*sum(wnew**2)\n",
    "        return wobj\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization inopposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/50} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.L1C,self.L2C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our modified version of the BFGS Binary Logistic Regression from class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C,regul):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,L1C,L2C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        if(L1C > 0 and L2C > 0):\n",
    "            gradient[1:] += -2 * (np.sign(w[1:]) * L1C + w[1:] * L2C)\n",
    "        elif(self.L1C > 0):\n",
    "            gradient[1:] += -2 * np.sign(w[1:]) * L1C \n",
    "        elif(self.L2C > 0):\n",
    "            gradient[1:] += (-2 * w[1:] * L2C)\n",
    "        else:\n",
    "            gradient[1:] += -2 * w[1:] \n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.L1C,self.L2C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our modified version of the Logistic Regression from class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20,method='none',LC1=0.01,LC2=0.01):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.method = method\n",
    "        self.LC1 = LC1\n",
    "        self.LC2 = LC2\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            if(self.method == 'none'):\n",
    "                blr = BinaryLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            elif(self.method == 'stoc'):\n",
    "                blr = StochasticLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            elif(self.method=='steep'):\n",
    "                blr=LineSearchLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            elif(self.method=='newt'):\n",
    "                blr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            print(X.shape)\n",
    "            print(y_binary.shape)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression\n",
    "\n",
    "### parameters go in order eta, iterations, method of regression, L1 Regularization Cost, L2 regularization Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13896, 25)\n",
      "(13896,)\n",
      "here\n",
      "(13896,)\n",
      "(13896, 13896)\n",
      "(13896, 26)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (13896,26) (13896,1,13896) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-88d6b3bc4657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time lr.fit(xTrain,yTrain)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</anaconda/lib/python3.6/site-packages/decorator.py:decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-f39a0093274f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_binary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mblr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m# add the trained classifier to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-bede0b926031>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'here'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m \u001b[0;31m# multiply by learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# add bacause maximizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-bede0b926031>\u001b[0m in \u001b[0;36m_get_gradient\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mydiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mydiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make ydiff a column vector and multiply through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gradient 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (13896,26) (13896,1,13896) "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(0.1,500,'none',0.01,0.01)\n",
    "%time lr.fit(xTrain,yTrain)\n",
    "\n",
    "yhat = lr.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.01,100,'steep',0.001,0.001)\n",
    "%time lr.fit(xTrain,yTrain)\n",
    "\n",
    "yhat = lr.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stochastic-gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.1,1000,'stoc',0.001,0.001)\n",
    "%time lr.fit(xTrain,yTrain)\n",
    "\n",
    "yhat = lr.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quasi newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.01,50,'newt',0.001,0.001)\n",
    "%time lr.fit(xTrain,yTrain)\n",
    "\n",
    "yhat = lr.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We tried every technique with every combination of 10 ,1 ,0.1 ,0.01 ,0.001, and 0.0001 for L1 and L2 regularization </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = ['none','steep','stoc','newt']\n",
    "vals = [10,1,0.1,0.01,0.001,0.0001]\n",
    "maxNone = [0,0,0]\n",
    "maxSteep = [0,0,0]\n",
    "maxStoc = [0,0,0]\n",
    "maxNewt = [0,0,0]\n",
    "maxVal = [0,0,0,'test']\n",
    "noney = [[],[],[],[],[],[]]\n",
    "steepy =  [[],[],[],[],[],[]]\n",
    "stocy =  [[],[],[],[],[],[]]\n",
    "newty =  [[],[],[],[],[],[]]\n",
    "\n",
    "for tech in techniques:\n",
    "    i = 0\n",
    "    for val in vals:\n",
    "        for val2 in vals:\n",
    "            lr = LogisticRegression(0.01,50,tech,val,val2)\n",
    "            lr.fit(xTrain,yTrain)\n",
    "            yhat = lr.predict(xTest)\n",
    "            score = accuracy_score(yTest,yhat)\n",
    "            if(tech == 'none'):\n",
    "                noney[i].append(score)\n",
    "                if(score > maxNone[0]):\n",
    "                    maxNone[0] = score\n",
    "                    maxNone[1] = val\n",
    "                    maxNone[2] = val2\n",
    "            if(tech == 'steep'):\n",
    "                steepy[i].append(score)\n",
    "                if(score > maxSteep[0]):\n",
    "                    maxSteep[0] = score\n",
    "                    maxSteep[1] = val\n",
    "                    maxSteep[2] = val2\n",
    "            if(tech == 'stoc'):\n",
    "                stocy[i].append(score)\n",
    "                if(score > maxStoc[0]):\n",
    "                    maxStoc[0] = score\n",
    "                    maxStoc[1] = val\n",
    "                    maxStoc[2] = val2\n",
    "            if(tech == 'newt'):\n",
    "                newty[i].append(score)\n",
    "                if(score > maxNewt[0]):\n",
    "                    maxNewt[0] = score\n",
    "                    maxNewt[1] = val\n",
    "                    maxNewt[2] = val2\n",
    "            if(score > maxVal[0]):\n",
    "                maxVal[0] = score\n",
    "                maxVal[1] = val\n",
    "                maxVal[2] = val2\n",
    "                maxVal[3] = tech\n",
    "        i+=1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneX = [10,1,0.1,0.01,0.001,0.0001]\n",
    "fig = plt.figure(1, figsize=(13,13))\n",
    "ax2  = fig.add_subplot(212)\n",
    "for i in noney:\n",
    "    plt.plot(noneX, i) #we plot y as a function of a, which parametrizes x\n",
    "ax2.set_xscale('log', basex=10)\n",
    "ax2.set_title('Binary Logisitic Regression')\n",
    "ax2.legend(['L1 = 10', 'L1 = 1', 'L1 = 0.1', 'L1 = 0.01','L1 = 0.001', 'L1 = 0.0001'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(13,13))\n",
    "ax2  = fig.add_subplot(212)\n",
    "for i in steepy:\n",
    "    ax2.plot(noneX, i) #we plot y as a function of a, which parametrizes x\n",
    "ax2.set_xscale('log', basex=10)\n",
    "ax2.set_title('Steepest Descent Regularization')\n",
    "ax2.legend(['L1 = 10', 'L1 = 1', 'L1 = 0.1', 'L1 = 0.01','L1 = 0.001', 'L1 = 0.0001'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(13,13))\n",
    "ax2  = fig.add_subplot(212)\n",
    "for i in stocy:\n",
    "    ax2.plot(noneX, i) #we plot y as a function of a, which parametrizes x\n",
    "ax2.set_xscale('log', basex=10)\n",
    "ax2.set_title('Stocastic-Gradient Descent Regularization')\n",
    "ax2.legend(['L1 = 10', 'L1 = 1', 'L1 = 0.1', 'L1 = 0.01','L1 = 0.001', 'L1 = 0.0001'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(13,13))\n",
    "ax2  = fig.add_subplot(212)\n",
    "for i in newty:\n",
    "    ax2.plot(noneX, i) #we plot y as a function of a, which parametrizes x\n",
    "ax2.set_xscale('log', basex=10)\n",
    "ax2.set_title('Newtons Method Regularization')\n",
    "ax2.legend(['L1 = 10', 'L1 = 1', 'L1 = 0.1', 'L1 = 0.01','L1 = 0.001', 'L1 = 0.0001'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max for regular lr', maxNone[0],' L1:',maxNone[1],' L2:',maxNone[2])\n",
    "print('max for steep', maxSteep[0],' L1:',maxSteep[1],' L2:',maxSteep[2])\n",
    "print('max for stocastic',  maxStoc[0],' L1:',maxStoc[1],' L2:',maxStoc[2])\n",
    "print('max for newton\\'s', maxNewt[0],' L1:',maxNewt[1],' L2:',maxNewt[2])\n",
    "print('max overall', maxVal[0],' L1:',maxVal[1],' L2:',maxVal[2], ' Techique:', maxVal[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> After testing each of our methods of regression, we found the steepest descent worked best on our data. Steepest Descent worked best with both L1 and L2 regularization. The cost for L1 was 1 and the cost for L2 was 0.001.   Stocastic and Binary Logistic regression were close in accuracy to Steepest Descent, which means we should keep them in mind as a method if we did more work on the dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We believe that this is data snooping because we are testing every combination and picking the best. Also, from the charts we can see that there is no real pattern based around cost value or method. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Our Implementation to SciKitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how do we compare now to sklearn?\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSci\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "#From Sklearn documentation \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = list(unique_labels(y_true, y_pred))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sk = LogisticRegressionSci(C=0.00001) # all params default\n",
    "# note that sklearn is optimized for using the liblinear library with logistic regression\n",
    "# ...and its faster than our implementation here\n",
    "lr_sk.fit(xTrain,yTrain) # no need to add bias term, sklearn does it internally!!\n",
    "yhat = lr_sk.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = ['low','medium','high']\n",
    "print(yhat)\n",
    "plot_confusion_matrix(yTest, yhat, classes=class_names,\n",
    "                      title='SciKit Learn Implementation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.01,50,'steep',1,0.001)\n",
    "%time lr.fit(xTrain,yTrain)\n",
    "\n",
    "yhat = lr.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))\n",
    "plot_confusion_matrix(yTest, yhat, classes=class_names,\n",
    "                      title='Our Implementation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, L1C=0.001,L2C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.L1C = L1C\n",
    "        self.L2C = L2C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.L1C > 0 and self.L2C > 0):\n",
    "            gradient[1:] += -2 * (np.sign(self.w_[1:]) * self.L1C + self.w_[1:] * self.L2C)\n",
    "        elif(self.L1C > 0):\n",
    "            gradient[1:] += -2 * np.sign(self.w_[1:]) * self.L1C \n",
    "        elif(self.L2C > 0):\n",
    "            gradient[1:] += (-2 * self.w_[1:] * self.L2C)\n",
    "        else:\n",
    "            gradient[1:] += -2 * self.w_[1:] \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "class LogisticRegressionEW:\n",
    "    def __init__(self, eta, iterations=20,method='none',LC1=0.01,LC2=0.01):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.method = method\n",
    "        self.LC1 = LC1\n",
    "        self.LC2 = LC2\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    def split_data(self,X,y,classes):\n",
    "        newY = []\n",
    "        newX = []\n",
    "        for i in range(0,classes):\n",
    "            newY.append([])\n",
    "            newX.append([])\n",
    "        for i in range(0,len(X)):\n",
    "            ans = y[i]\n",
    "            newX[ans].append(X[i])\n",
    "        return newX\n",
    "            \n",
    "            \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        data = self.split_data(X,y,num_unique_classes)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        pairs = [\"\".join(map(str, comb)) for comb in combinations(self.unique_, 2)]\n",
    "        temp = []\n",
    "        for i in pairs:\n",
    "            temp.append((int(i[0]),int(i[1])))\n",
    "        pairs = temp\n",
    "        for i in pairs: # for each unique value\n",
    "            # train the binary classifier for this class\n",
    "            if(self.method == 'none'):\n",
    "                blr = BinaryLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            elif(self.method == 'stoc'):\n",
    "                blr = StochasticLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            elif(self.method=='steep'):\n",
    "                blr=LineSearchLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            elif(self.method=='newt'):\n",
    "                blr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.LC1,self.LC2)\n",
    "            X = np.vstack([data[i[0]],data[i[1]]])\n",
    "            y = np.concatenate((np.full((len(data[i[0]]), 1), i[0]),np.full((len(data[i[1]]), 1), i[1])),axis=None)\n",
    "            print(i)\n",
    "            blr.fit(X,y)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append((blr,i))\n",
    "        print(self.classifiers_)\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X[1])) # get probability for each classifier\n",
    "            \n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        newX = []\n",
    "        for i,val in enumerate(X):\n",
    "            newX.append((i,val))\n",
    "        i = 0 \n",
    "        while(i < len(self.classifiers_)):\n",
    "            \n",
    "\n",
    "#         return (self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(0, 2)\n",
      "(1, 2)\n",
      "[<__main__.BinaryLogisticRegression object at 0x114721c50>, <__main__.BinaryLogisticRegression object at 0x1147217b8>, <__main__.BinaryLogisticRegression object at 0x175ca86a0>]\n",
      "CPU times: user 2.93 s, sys: 335 ms, total: 3.26 s\n",
      "Wall time: 1.66 s\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-818768baeb97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time lr.fit(xTrain,yTrain)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-9b56f486c266>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;31m#         return (self.predict_proba(X),axis=1) # take argmax along row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegressionEW(0.1,500,'none',0.01,0.01)\n",
    "%time lr.fit(xTrain,yTrain)\n",
    "\n",
    "yhat = lr.predict(xTest)\n",
    "print('Accuracy of: ',accuracy_score(yTest,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(13896, 25)\n",
    "(13896,)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
