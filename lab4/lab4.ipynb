{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Our task for this lab is to create our own logistic regression model which is able to classify how many Uber pickups there will be (low, medium, or high) based off of different information in our dataset. The dataset is a collection of information about Uber pickups like time and location, joined with other data such as the weather for that time and location, what borough it is in, and whether or not it was a NYC public holiday. We split our predictions up by borough because certain boroughs like Manhattan generally always have a higher volume of pickups than boroughs like the Bronx, so aggregate predictions over all of NYC would not have been very insightful. Instead, we make predictions specific to each borough, with the exception of EWR and Staten Island, which we threw out because they did not contain enough data to make accurate predictions. We denote a \"high\" amount of pickups as greater than half a standard deviation above the mean for that borough. A \"low\" amount is less than half a standard deviation below the mean for that borough. A \"medium\" amount is inbetween. \n",
    "\n",
    "Our prediction task is valuable because it gives Uber insight into the time periods where they can be most profitable, and time periods where they can save money. For example, on New Years Eve there is most likely an extreme surge in the number of rides requested. If there are not enough drivers to satisfy all of these rides, people will go to Lyft or even just hail a yellow cab. However, if they prepare for this surge by incentivising drivers with an extra percentage of the ride money, there will be more drivers to satisfy the extra rides requests. Our model's insights would help pull in more profits and increases market share compared to treating every day and location as equally profitable. In production, this model would provide the best results if it were deployed so that it would run constantly and react to changing weather conditions, social movements, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/uber_nyc_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>490.215903</td>\n",
       "      <td>5.984924</td>\n",
       "      <td>8.818125</td>\n",
       "      <td>47.669042</td>\n",
       "      <td>30.823065</td>\n",
       "      <td>1017.817938</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.090464</td>\n",
       "      <td>2.529169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>995.649536</td>\n",
       "      <td>3.699007</td>\n",
       "      <td>2.442897</td>\n",
       "      <td>19.814969</td>\n",
       "      <td>21.283444</td>\n",
       "      <td>7.768796</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.093125</td>\n",
       "      <td>0.219402</td>\n",
       "      <td>4.520325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>991.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1012.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>449.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1022.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>2.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7883.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1043.400000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickups           spd           vsb          temp          dewp  \\\n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000   \n",
       "mean     490.215903      5.984924      8.818125     47.669042     30.823065   \n",
       "std      995.649536      3.699007      2.442897     19.814969     21.283444   \n",
       "min        0.000000      0.000000      0.000000      2.000000    -16.000000   \n",
       "25%        1.000000      3.000000      9.100000     32.000000     14.000000   \n",
       "50%       54.000000      6.000000     10.000000     46.000000     30.000000   \n",
       "75%      449.000000      8.000000     10.000000     64.500000     50.000000   \n",
       "max     7883.000000     21.000000     10.000000     89.000000     73.000000   \n",
       "\n",
       "                slp         pcp01         pcp06         pcp24            sd  \n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000  \n",
       "mean    1017.817938      0.003830      0.026129      0.090464      2.529169  \n",
       "std        7.768796      0.018933      0.093125      0.219402      4.520325  \n",
       "min      991.400000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%     1012.500000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%     1018.200000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%     1022.900000      0.000000      0.000000      0.050000      2.958333  \n",
       "max     1043.400000      0.280000      1.240000      2.100000     19.000000  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> checking for nan or null values in the dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found only Borough has nan values so we remove the nan rows </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough       True\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough      False\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that most of our data didnt have much correlation except temperate and the dew point temperature. We decided to get rid of this variable becasue it seemed very similar to temperature and did not think it would impact the machine learning. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pickups</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.015708</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.002821</td>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.009676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spd</th>\n",
       "      <td>0.009741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.097041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vsb</th>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.047834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.063692</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>-0.545558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dewp</th>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.489372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slp</th>\n",
       "      <td>-0.015708</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.121508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp01</th>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp06</th>\n",
       "      <td>-0.002821</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.039943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp24</th>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>-0.009676</td>\n",
       "      <td>0.097041</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.545558</td>\n",
       "      <td>-0.489372</td>\n",
       "      <td>0.121508</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>0.069664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickups       spd       vsb      temp      dewp       slp     pcp01  \\\n",
       "pickups  1.000000  0.009741 -0.008429  0.063692  0.040082 -0.015708  0.005007   \n",
       "spd      0.009741  1.000000  0.086177 -0.296126 -0.321606 -0.092761 -0.000357   \n",
       "vsb     -0.008429  0.086177  1.000000  0.025214 -0.231294  0.167039 -0.488407   \n",
       "temp     0.063692 -0.296126  0.025214  1.000000  0.896544 -0.224537 -0.013343   \n",
       "dewp     0.040082 -0.321606 -0.231294  0.896544  1.000000 -0.311156  0.115399   \n",
       "slp     -0.015708 -0.092761  0.167039 -0.224537 -0.311156  1.000000 -0.089752   \n",
       "pcp01    0.005007 -0.000357 -0.488407 -0.013343  0.115399 -0.089752  1.000000   \n",
       "pcp06   -0.002821  0.016668 -0.118346 -0.037295  0.013293 -0.104940  0.128064   \n",
       "pcp24   -0.022935 -0.010412  0.000895 -0.014408  0.001519 -0.134689  0.000997   \n",
       "sd      -0.009676  0.097041 -0.047834 -0.545558 -0.489372  0.121508  0.000310   \n",
       "\n",
       "            pcp06     pcp24        sd  \n",
       "pickups -0.002821 -0.022935 -0.009676  \n",
       "spd      0.016668 -0.010412  0.097041  \n",
       "vsb     -0.118346  0.000895 -0.047834  \n",
       "temp    -0.037295 -0.014408 -0.545558  \n",
       "dewp     0.013293  0.001519 -0.489372  \n",
       "slp     -0.104940 -0.134689  0.121508  \n",
       "pcp01    0.128064  0.000997  0.000310  \n",
       "pcp06    1.000000  0.251166  0.039943  \n",
       "pcp24    0.251166  1.000000  0.069664  \n",
       "sd       0.039943  0.069664  1.000000  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['dewp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We made the holiday column count 1 for yes and 0 for no. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hday'] = data['hday'].apply(lambda x: 0 if x=='N' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We one hot encoded our boroughs becuase they were string values </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotCols = pd.get_dummies(data['borough'])\n",
    "data = data.join(oneHotCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['borough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the time of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We based our hour groups by sunrise and sunset. Night is the time when the sun is down, which on average is from 8pm to 6am. Morning is from 6am till noon. Afternoon is from noon till 5pm. Evening is from 5pm till 8pm, which is around when the sunsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dateTest = data['pickup_dt'][0]\n",
    "print(int(dateTest[11:13]))\n",
    "data['is_morning'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 6 and int(x[11:13]) < 12) else 0)\n",
    "data['is_afternoon'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 12 and int(x[11:13]) < 17) else 0)\n",
    "data['is_evening'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 17 and int(x[11:13]) < 21) else 0)\n",
    "data['is_night']  = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 21 or int(x[11:13]) < 6) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the weekday\n",
    "\n",
    "The weekday from the pickup_dt feature has been 1 hot encoded into monday-sunday. We believe having each day as a feature will help classify & predict the number of ubers necessary at a future date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "data['is_monday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 0 else 0)\n",
    "data['is_tuesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 1 else 0)\n",
    "data['is_wednesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 2 else 0)\n",
    "data['is_thursday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 3 else 0)\n",
    "data['is_friday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 4 else 0)\n",
    "data['is_saturday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 5 else 0)\n",
    "data['is_sunday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['pickup_dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that the borough EWR averages about 2.4 pickups every 96 hours so we are getting rid of the EWR borough from our dataset. We found that the borough Staten Island averages 1.6 pickups and hour and had a max 13 pickups in an hour over 6 months so we got rid of it from our dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           pickups     EWR\n",
      "count  4343.000000  4343.0\n",
      "mean      0.024177     1.0\n",
      "std       0.160937     0.0\n",
      "min       0.000000     1.0\n",
      "25%       0.000000     1.0\n",
      "50%       0.000000     1.0\n",
      "75%       0.000000     1.0\n",
      "max       2.000000     1.0\n",
      "           pickups  Staten Island\n",
      "count  4343.000000         4343.0\n",
      "mean      1.601888            1.0\n",
      "std       1.640451            0.0\n",
      "min       0.000000            1.0\n",
      "25%       0.000000            1.0\n",
      "50%       1.000000            1.0\n",
      "75%       2.000000            1.0\n",
      "max      13.000000            1.0\n"
     ]
    }
   ],
   "source": [
    "d1 = data.where(data['EWR']==1)[['pickups','EWR']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data.EWR == 0]\n",
    "del data['EWR']\n",
    "d1 = data.where(data['Staten Island']==1)[['pickups','Staten Island']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data['Staten Island'] == 0]\n",
    "del data['Staten Island']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We have three cateogries of pickup traffic low, medium, high. We found these by finding the mean and standard deviation of each borough. Low is half a standard deviation below the mean and high is half a stadard deviation above the mean. Anything else is counted as a medium amount of pickups. This means that the category of low, medium, and high pickup amount depends on the borough. If we did base our categories by borough then Manhattan would always be in the high pickup amount category and Queens would always be in the low pickup amount category.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4343.000000\n",
      "mean     2387.253281\n",
      "std      1434.724668\n",
      "min         0.000000\n",
      "25%      1223.500000\n",
      "50%      2269.000000\n",
      "75%      3293.500000\n",
      "max      7883.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean       50.667050\n",
      "std        31.029223\n",
      "min         0.000000\n",
      "25%        29.000000\n",
      "50%        46.000000\n",
      "75%        66.000000\n",
      "max       262.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      309.354824\n",
      "std       154.368300\n",
      "min         0.000000\n",
      "25%       196.000000\n",
      "50%       308.000000\n",
      "75%       410.000000\n",
      "max       831.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      534.431269\n",
      "std       294.810182\n",
      "min         0.000000\n",
      "25%       331.500000\n",
      "50%       493.000000\n",
      "75%       675.000000\n",
      "max      2009.000000\n",
      "Name: pickups, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Man = data[data.Manhattan == 1]\n",
    "Bronx = data[data.Bronx == 1]\n",
    "Queens = data[data.Queens == 1]\n",
    "Brooklyn = data[data.Brooklyn == 1]\n",
    "\n",
    "print(Man['pickups'].describe())\n",
    "print(Bronx['pickups'].describe())\n",
    "print(Queens['pickups'].describe())\n",
    "print(Brooklyn['pickups'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Using the mean and stadard deviation of each borough to place each row of data into a category. We can see that our categories are almost completely balanced within each borough.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    17372.000000\n",
      "mean         0.935816\n",
      "std          0.771724\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          2.000000\n",
      "max          2.000000\n",
      "Name: pickupPrediction, dtype: float64\n",
      "1    6955\n",
      "0    5766\n",
      "2    4651\n",
      "Name: pickupPrediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mstd = Man['pickups'].std()\n",
    "mmean = Man['pickups'].mean()\n",
    "bstd = Bronx['pickups'].std()\n",
    "bmean = Bronx['pickups'].mean()\n",
    "qstd = Queens['pickups'].std()\n",
    "qmean = Queens['pickups'].mean()\n",
    "brstd = Brooklyn['pickups'].std()\n",
    "brmean = Brooklyn['pickups'].mean()\n",
    "data['pickupPrediction'] = 0\n",
    "for index, row in data.iterrows():\n",
    "    if(row['Manhattan'] == 1):\n",
    "        if(row['pickups']  < (mmean - mstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (mmean + mstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Bronx'] == 1):\n",
    "        if(row['pickups']  < (bmean - bstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (bmean + bstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Queens'] == 1):\n",
    "        if(row['pickups']  < (qmean - qstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (qmean + qstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Brooklyn'] == 1):\n",
    "        if(row['pickups']  < (brmean - brstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (brmean + brstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "print(data['pickupPrediction'].describe())\n",
    "print(data['pickupPrediction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True, drop=True)\n",
    "for index, row in data.iterrows():\n",
    "    if(index < 12):\n",
    "        continue\n",
    "    data.set_value(index,'1hrAgo',data['pickupPrediction'][index-4])\n",
    "    data.set_value(index,'2hrsAgo',data['pickupPrediction'][index-8])\n",
    "    data.set_value(index,'3hrsAgo',data['pickupPrediction'][index-12])\n",
    "data = data.iloc[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = data['pickupPrediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del x['pickupPrediction']\n",
    "del x['pickups']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "x = x.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "x = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Used\n",
    "<p> We believe using an f1-socre would be the best evaluation metric in our case. We need to take into account the false positive and false negative rate because our logisitc regression on the algorithm would always guesses one class which results in a precision of 100% for that class. We care about any form of misclassification of our pickup class prediction. We will give false negatives a higher cost because this means we have too few ubers in an area based off our preditcion. If we have a false positive then it means we have too many ubers in an area, which is bad, but not as bad as having too few. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "#                     ('clf', LogisticRegression())])\n",
    "\n",
    "# pipe_lr.fit(xTrain, yTrain)\n",
    "# y_pred = pipe_lr.predict(xTest)\n",
    "# print(classification_report(yTest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "We choose to use continuous test and training sets as our uber pickups were given hourly over a 6 month span. Our algorithm in the real world would be getting the data in hourly and then have to predict based off this new hourly data. We split our data into 10 folds splitting our data by the time that it occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "      <td>17360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.285771</td>\n",
       "      <td>0.881921</td>\n",
       "      <td>0.523001</td>\n",
       "      <td>0.507870</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.043380</td>\n",
       "      <td>0.133592</td>\n",
       "      <td>0.037788</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250230</td>\n",
       "      <td>0.208525</td>\n",
       "      <td>0.166820</td>\n",
       "      <td>0.374424</td>\n",
       "      <td>0.143779</td>\n",
       "      <td>0.143779</td>\n",
       "      <td>0.138249</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.143779</td>\n",
       "      <td>0.143779</td>\n",
       "      <td>0.143779</td>\n",
       "      <td>0.467569</td>\n",
       "      <td>0.467569</td>\n",
       "      <td>0.467598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.176538</td>\n",
       "      <td>0.244367</td>\n",
       "      <td>0.227285</td>\n",
       "      <td>0.149713</td>\n",
       "      <td>0.067286</td>\n",
       "      <td>0.074957</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.237982</td>\n",
       "      <td>0.190689</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433158</td>\n",
       "      <td>0.406266</td>\n",
       "      <td>0.372826</td>\n",
       "      <td>0.483988</td>\n",
       "      <td>0.350875</td>\n",
       "      <td>0.350875</td>\n",
       "      <td>0.345171</td>\n",
       "      <td>0.349937</td>\n",
       "      <td>0.350875</td>\n",
       "      <td>0.350875</td>\n",
       "      <td>0.350875</td>\n",
       "      <td>0.385763</td>\n",
       "      <td>0.385763</td>\n",
       "      <td>0.385784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.342146</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500958</td>\n",
       "      <td>0.515385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.712644</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.167763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  17360.000000  17360.000000  17360.000000  17360.000000  17360.000000   \n",
       "mean       0.285771      0.881921      0.523001      0.507870      0.013655   \n",
       "std        0.176538      0.244367      0.227285      0.149713      0.067286   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.142857      0.910000      0.342146      0.403846      0.000000   \n",
       "50%        0.285714      1.000000      0.500958      0.515385      0.000000   \n",
       "75%        0.380952      1.000000      0.712644      0.605769      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  17360.000000  17360.000000  17360.000000  17360.000000  17360.000000   \n",
       "mean       0.021042      0.043380      0.133592      0.037788      0.250000   \n",
       "std        0.074957      0.105000      0.237982      0.190689      0.433025   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.024603      0.167763      0.000000      0.250000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 10            11            12            13            14  \\\n",
       "count  17360.000000  17360.000000  17360.000000  17360.000000  17360.000000   \n",
       "mean       0.250000      0.250000      0.250000      0.250230      0.208525   \n",
       "std        0.433025      0.433025      0.433025      0.433158      0.406266   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.250000      0.250000      0.250000      1.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 15            16            17            18            19  \\\n",
       "count  17360.000000  17360.000000  17360.000000  17360.000000  17360.000000   \n",
       "mean       0.166820      0.374424      0.143779      0.143779      0.138249   \n",
       "std        0.372826      0.483988      0.350875      0.350875      0.345171   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 20            21            22            23            24  \\\n",
       "count  17360.000000  17360.000000  17360.000000  17360.000000  17360.000000   \n",
       "mean       0.142857      0.143779      0.143779      0.143779      0.467569   \n",
       "std        0.349937      0.350875      0.350875      0.350875      0.385763   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.500000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 25            26  \n",
       "count  17360.000000  17360.000000  \n",
       "mean       0.467569      0.467598  \n",
       "std        0.385763      0.385784  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%        0.500000      0.500000  \n",
       "75%        1.000000      1.000000  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reset_index(inplace=True, drop=True)\n",
    "y.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The data was already in order by hour. Each four rows represented one boroughs uber and weather data. We split our data into 10 folds for training. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 2892 2893 2894] TEST: [2895 2896 2897 ... 5785 5786 5787]\n",
      "TRAIN: [   0    1    2 ... 5785 5786 5787] TEST: [5788 5789 5790 ... 8678 8679 8680]\n",
      "TRAIN: [   0    1    2 ... 8678 8679 8680] TEST: [ 8681  8682  8683 ... 11571 11572 11573]\n",
      "TRAIN: [    0     1     2 ... 11571 11572 11573] TEST: [11574 11575 11576 ... 14464 14465 14466]\n",
      "TRAIN: [    0     1     2 ... 14464 14465 14466] TEST: [14467 14468 14469 ... 17357 17358 17359]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(x.values,y.values):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 1/10, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how do we compare now to sklearn?\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSci\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "#From Sklearn documentation \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = list(unique_labels(y_true, y_pred))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                act_func='sigmoid',cost_func='quadratic',layers=2):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.act_func = act_func\n",
    "        self.cost_func = cost_func\n",
    "        self.layers = layers\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        W = []\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        W.append(W1)\n",
    "        for i in range(0,self.layers-2):\n",
    "            W1_num_elems = (self.n_hidden + 1)*self.n_hidden\n",
    "            W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "            W1 = W1.reshape(self.n_hidden, self.n_hidden + 1) # reshape to be W\n",
    "            W.append(W1)\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        W.append(W2)\n",
    "        return W\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        total = 0 \n",
    "        for i in W:\n",
    "            total += np.mean(i[:, 1:] ** 2)\n",
    "        return (lambda_/2.0) * np.sqrt(total)\n",
    "    \n",
    "    def _cost(self,A2,Y_enc,W):\n",
    "        '''Get the objective function value'''\n",
    "        if(self.cost_func == 'quadratic'):\n",
    "            cost = np.mean((Y_enc-A2)**2)\n",
    "        else:\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(A2)+(1-Y_enc)*np.log(1-A2))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        if(self.act_func == 'sigmoid'):\n",
    "            A2 = self._sigmoid(Z1)\n",
    "        else:\n",
    "            A2 = Z1\n",
    "#         A2 = self._add_bias_unit(A2, how='row')\n",
    "        return A1, Z1, A2\n",
    "    \n",
    "    def _get_gradient(self, A, Z, Y_enc, W):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        ones = np.ones(self.n_hidden + 1)\n",
    "        grads = []\n",
    "        # for each instance's activations \n",
    "        for i2 in range(len(self.W)-1,-1,-1):\n",
    "            for (a,a2,y) in zip(A[i2+1].T,A[i2].T,Y_enc.T):\n",
    "                grad1 = np.zeros(W[i2].shape)\n",
    "                if(i2 == len(self.W)-1):\n",
    "                    if(self.cost_func == 'quadratic'):\n",
    "                        dJ_dz2 = -2*(y - a)*a*(1-a)\n",
    "                    else:\n",
    "                        dJ_dz2 = a-y\n",
    "                    grad1 += dJ_dz2[:,np.newaxis]  @ a2[np.newaxis,:]\n",
    "                else:\n",
    "                    if(self.act_func == 'sigmoid'):\n",
    "                        dJ_dz1 = dJ_dz2 @ W[i2+1] @ np.diag(a*(1-a))\n",
    "                    else:\n",
    "                        dJ_dz1 = dJ_dz2 @ W[i2+1] @ np.diag(ones)\n",
    "                    grad1 += dJ_dz1[1:,np.newaxis] @ a2[np.newaxis,:] \n",
    "            grads.insert(0,grad1)\n",
    "            if(i2 < len(self.W)-1):\n",
    "                dJ_dz2 = dJ_dz1[1:]\n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "#         # regularize weights that are not bias terms\n",
    "        for i in range(0,len(W)):\n",
    "            grads[i][:, 1:] += (W[i][:, 1:] * self.l2_C)\n",
    "        return grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        for i in range(0,len(self.W)):\n",
    "            if(i>0):\n",
    "                X = A2.T\n",
    "            A1, Z1, A2 = self._feedforward(X,self.W[i])\n",
    "        y_pred = np.argmax(A2, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "            # feedforward all instances\n",
    "            A = []\n",
    "            Z = []\n",
    "            X_data = X.copy()\n",
    "            for i2 in range(0,len(self.W)):\n",
    "                W1 = self.W[i2]\n",
    "                if(i2>0):\n",
    "                    X_data = A2.T\n",
    "                A1, Z1, A2 = self._feedforward(X_data,W1)\n",
    "                if(i2 == 0):\n",
    "                    A.append(A1)\n",
    "                if(i2 != len(self.W)-1):\n",
    "                    A3 = self._add_bias_unit(A2, how='row')\n",
    "                    A.append(A3)\n",
    "                else:\n",
    "                    A.append(A2)\n",
    "                Z.append(Z1)\n",
    "            cost = self._cost(A[len(A)-1],Y_enc,self.W)\n",
    "            self.cost_.append(cost)\n",
    "            # compute gradient via backpropagation\n",
    "            ## return an array of gradients to update weight array\n",
    "            ## iterate inside of get gradient function instead\n",
    "            grads = self._get_gradient(A=A, Z=Z,Y_enc=Y_enc, W=self.W)\n",
    "            for i in range(0,len(grads)):\n",
    "                self.W[i] -= self.eta * grads[i]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50, \n",
    "                C=0.1,\n",
    "                epochs=100,\n",
    "                eta=0.001,\n",
    "                random_state=1,\n",
    "                act_func='sigmoid',\n",
    "                cost_func='quadratic',\n",
    "                layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 65  15 923]\n",
      " [298  48 932]\n",
      " [256  19 337]]\n",
      "Test acc: [0.08014797 0.07058824 0.2403709 ]\n",
      "fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "Epoch: 100/100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[   0    0 1043]\n",
      " [   0    0 1310]\n",
      " [   0    0  540]]\n",
      "Test acc: [0.         0.         0.31459365]\n",
      "fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "Epoch: 100/100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[   0    0  864]\n",
      " [   0    0 1363]\n",
      " [   0    0  666]]\n",
      "Test acc: [0.         0.         0.37426243]\n",
      "fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[  0  75 574]\n",
      " [  0 140 912]\n",
      " [  0 296 896]]\n",
      "Test acc: [0.         0.17914267 0.50139899]\n",
      "fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[   0  572    0]\n",
      " [   0  968    0]\n",
      " [   0 1353    0]]\n",
      "Test acc: [0.        0.5014245 0.       ]\n",
      "CPU times: user 6min 55s, sys: 19.7 s, total: 7min 14s\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "nn = TwoLayerPerceptron(**params)\n",
    "i = 1\n",
    "for train_index, test_index in tscv.split(x.values,y.values):\n",
    "    print('fold', i)\n",
    "    X_train, X_test = x.iloc[train_index].values, x.iloc[test_index].values\n",
    "    y_train, y_test = y.iloc[train_index].values, y.iloc[test_index].values\n",
    "    nn.fit(X_train, y_train, print_progress=10)\n",
    "    yhat = nn.predict(X_test)\n",
    "    class_names = ['low','medium','high']\n",
    "    plot_confusion_matrix(y_test, yhat, classes=class_names,\n",
    "                      title='SciKit Learn Implementation')\n",
    "    print('Test acc:',f1_score(y_test,yhat,average=None))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
