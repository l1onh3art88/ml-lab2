{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Our task for this lab is to create our own logistic regression model which is able to classify how many Uber pickups there will be (low, medium, or high) based off of different information in our dataset. The dataset is a collection of information about Uber pickups like time and location, joined with other data such as the weather for that time and location, what borough it is in, and whether or not it was a NYC public holiday. We split our predictions up by borough because certain boroughs like Manhattan generally always have a higher volume of pickups than boroughs like the Bronx, so aggregate predictions over all of NYC would not have been very insightful. Instead, we make predictions specific to each borough, with the exception of EWR and Staten Island, which we threw out because they did not contain enough data to make accurate predictions. We denote a \"high\" amount of pickups as greater than half a standard deviation above the mean for that borough. A \"low\" amount is less than half a standard deviation below the mean for that borough. A \"medium\" amount is inbetween. \n",
    "\n",
    "Our prediction task is valuable because it gives Uber insight into the time periods where they can be most profitable, and time periods where they can save money. For example, on New Years Eve there is most likely an extreme surge in the number of rides requested. If there are not enough drivers to satisfy all of these rides, people will go to Lyft or even just hail a yellow cab. However, if they prepare for this surge by incentivising drivers with an extra percentage of the ride money, there will be more drivers to satisfy the extra rides requests. Our model's insights would help pull in more profits and increases market share compared to treating every day and location as equally profitable. In production, this model would provide the best results if it were deployed so that it would run constantly and react to changing weather conditions, social movements, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/uber_nyc_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>490.215903</td>\n",
       "      <td>5.984924</td>\n",
       "      <td>8.818125</td>\n",
       "      <td>47.669042</td>\n",
       "      <td>30.823065</td>\n",
       "      <td>1017.817938</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.090464</td>\n",
       "      <td>2.529169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>995.649536</td>\n",
       "      <td>3.699007</td>\n",
       "      <td>2.442897</td>\n",
       "      <td>19.814969</td>\n",
       "      <td>21.283444</td>\n",
       "      <td>7.768796</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.093125</td>\n",
       "      <td>0.219402</td>\n",
       "      <td>4.520325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>991.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1012.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>449.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1022.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>2.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7883.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1043.400000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickups           spd           vsb          temp          dewp  \\\n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000   \n",
       "mean     490.215903      5.984924      8.818125     47.669042     30.823065   \n",
       "std      995.649536      3.699007      2.442897     19.814969     21.283444   \n",
       "min        0.000000      0.000000      0.000000      2.000000    -16.000000   \n",
       "25%        1.000000      3.000000      9.100000     32.000000     14.000000   \n",
       "50%       54.000000      6.000000     10.000000     46.000000     30.000000   \n",
       "75%      449.000000      8.000000     10.000000     64.500000     50.000000   \n",
       "max     7883.000000     21.000000     10.000000     89.000000     73.000000   \n",
       "\n",
       "                slp         pcp01         pcp06         pcp24            sd  \n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000  \n",
       "mean    1017.817938      0.003830      0.026129      0.090464      2.529169  \n",
       "std        7.768796      0.018933      0.093125      0.219402      4.520325  \n",
       "min      991.400000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%     1012.500000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%     1018.200000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%     1022.900000      0.000000      0.000000      0.050000      2.958333  \n",
       "max     1043.400000      0.280000      1.240000      2.100000     19.000000  "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> checking for nan or null values in the dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found only Borough has nan values so we remove the nan rows </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough       True\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough      False\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that most of our data didnt have much correlation except temperate and the dew point temperature. We decided to get rid of this variable becasue it seemed very similar to temperature and did not think it would impact the machine learning. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pickups</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.015708</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.002821</td>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.009676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spd</th>\n",
       "      <td>0.009741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.097041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vsb</th>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.047834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.063692</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>-0.545558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dewp</th>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.489372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slp</th>\n",
       "      <td>-0.015708</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.121508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp01</th>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp06</th>\n",
       "      <td>-0.002821</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.039943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp24</th>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>-0.009676</td>\n",
       "      <td>0.097041</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.545558</td>\n",
       "      <td>-0.489372</td>\n",
       "      <td>0.121508</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>0.069664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickups       spd       vsb      temp      dewp       slp     pcp01  \\\n",
       "pickups  1.000000  0.009741 -0.008429  0.063692  0.040082 -0.015708  0.005007   \n",
       "spd      0.009741  1.000000  0.086177 -0.296126 -0.321606 -0.092761 -0.000357   \n",
       "vsb     -0.008429  0.086177  1.000000  0.025214 -0.231294  0.167039 -0.488407   \n",
       "temp     0.063692 -0.296126  0.025214  1.000000  0.896544 -0.224537 -0.013343   \n",
       "dewp     0.040082 -0.321606 -0.231294  0.896544  1.000000 -0.311156  0.115399   \n",
       "slp     -0.015708 -0.092761  0.167039 -0.224537 -0.311156  1.000000 -0.089752   \n",
       "pcp01    0.005007 -0.000357 -0.488407 -0.013343  0.115399 -0.089752  1.000000   \n",
       "pcp06   -0.002821  0.016668 -0.118346 -0.037295  0.013293 -0.104940  0.128064   \n",
       "pcp24   -0.022935 -0.010412  0.000895 -0.014408  0.001519 -0.134689  0.000997   \n",
       "sd      -0.009676  0.097041 -0.047834 -0.545558 -0.489372  0.121508  0.000310   \n",
       "\n",
       "            pcp06     pcp24        sd  \n",
       "pickups -0.002821 -0.022935 -0.009676  \n",
       "spd      0.016668 -0.010412  0.097041  \n",
       "vsb     -0.118346  0.000895 -0.047834  \n",
       "temp    -0.037295 -0.014408 -0.545558  \n",
       "dewp     0.013293  0.001519 -0.489372  \n",
       "slp     -0.104940 -0.134689  0.121508  \n",
       "pcp01    0.128064  0.000997  0.000310  \n",
       "pcp06    1.000000  0.251166  0.039943  \n",
       "pcp24    0.251166  1.000000  0.069664  \n",
       "sd       0.039943  0.069664  1.000000  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['dewp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We made the holiday column count 1 for yes and 0 for no. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hday'] = data['hday'].apply(lambda x: 0 if x=='N' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We one hot encoded our boroughs becuase they were string values </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotCols = pd.get_dummies(data['borough'])\n",
    "data = data.join(oneHotCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['borough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the time of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We based our hour groups by sunrise and sunset. Night is the time when the sun is down, which on average is from 8pm to 6am. Morning is from 6am till noon. Afternoon is from noon till 5pm. Evening is from 5pm till 8pm, which is around when the sunsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dateTest = data['pickup_dt'][0]\n",
    "print(int(dateTest[11:13]))\n",
    "data['is_morning'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 6 and int(x[11:13]) < 12) else 0)\n",
    "data['is_afternoon'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 12 and int(x[11:13]) < 17) else 0)\n",
    "data['is_evening'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 17 and int(x[11:13]) < 21) else 0)\n",
    "data['is_night']  = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 21 or int(x[11:13]) < 6) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the weekday\n",
    "\n",
    "The weekday from the pickup_dt feature has been 1 hot encoded into monday-sunday. We believe having each day as a feature will help classify & predict the number of ubers necessary at a future date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "data['is_monday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 0 else 0)\n",
    "data['is_tuesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 1 else 0)\n",
    "data['is_wednesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 2 else 0)\n",
    "data['is_thursday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 3 else 0)\n",
    "data['is_friday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 4 else 0)\n",
    "data['is_saturday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 5 else 0)\n",
    "data['is_sunday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['pickup_dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that the borough EWR averages about 2.4 pickups every 96 hours so we are getting rid of the EWR borough from our dataset. We found that the borough Staten Island averages 1.6 pickups and hour and had a max 13 pickups in an hour over 6 months so we got rid of it from our dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           pickups     EWR\n",
      "count  4343.000000  4343.0\n",
      "mean      0.024177     1.0\n",
      "std       0.160937     0.0\n",
      "min       0.000000     1.0\n",
      "25%       0.000000     1.0\n",
      "50%       0.000000     1.0\n",
      "75%       0.000000     1.0\n",
      "max       2.000000     1.0\n",
      "           pickups  Staten Island\n",
      "count  4343.000000         4343.0\n",
      "mean      1.601888            1.0\n",
      "std       1.640451            0.0\n",
      "min       0.000000            1.0\n",
      "25%       0.000000            1.0\n",
      "50%       1.000000            1.0\n",
      "75%       2.000000            1.0\n",
      "max      13.000000            1.0\n"
     ]
    }
   ],
   "source": [
    "d1 = data.where(data['EWR']==1)[['pickups','EWR']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data.EWR == 0]\n",
    "del data['EWR']\n",
    "d1 = data.where(data['Staten Island']==1)[['pickups','Staten Island']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data['Staten Island'] == 0]\n",
    "del data['Staten Island']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We have three cateogries of pickup traffic low, medium, high. We found these by finding the mean and standard deviation of each borough. Low is half a standard deviation below the mean and high is half a stadard deviation above the mean. Anything else is counted as a medium amount of pickups. This means that the category of low, medium, and high pickup amount depends on the borough. If we did base our categories by borough then Manhattan would always be in the high pickup amount category and Queens would always be in the low pickup amount category.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4343.000000\n",
      "mean     2387.253281\n",
      "std      1434.724668\n",
      "min         0.000000\n",
      "25%      1223.500000\n",
      "50%      2269.000000\n",
      "75%      3293.500000\n",
      "max      7883.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean       50.667050\n",
      "std        31.029223\n",
      "min         0.000000\n",
      "25%        29.000000\n",
      "50%        46.000000\n",
      "75%        66.000000\n",
      "max       262.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      309.354824\n",
      "std       154.368300\n",
      "min         0.000000\n",
      "25%       196.000000\n",
      "50%       308.000000\n",
      "75%       410.000000\n",
      "max       831.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      534.431269\n",
      "std       294.810182\n",
      "min         0.000000\n",
      "25%       331.500000\n",
      "50%       493.000000\n",
      "75%       675.000000\n",
      "max      2009.000000\n",
      "Name: pickups, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Man = data[data.Manhattan == 1]\n",
    "Bronx = data[data.Bronx == 1]\n",
    "Queens = data[data.Queens == 1]\n",
    "Brooklyn = data[data.Brooklyn == 1]\n",
    "\n",
    "print(Man['pickups'].describe())\n",
    "print(Bronx['pickups'].describe())\n",
    "print(Queens['pickups'].describe())\n",
    "print(Brooklyn['pickups'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Using the mean and stadard deviation of each borough to place each row of data into a category. We can see that our categories are almost completely balanced within each borough.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    17372.000000\n",
      "mean         0.935816\n",
      "std          0.771724\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          2.000000\n",
      "max          2.000000\n",
      "Name: pickupPrediction, dtype: float64\n",
      "1    6955\n",
      "0    5766\n",
      "2    4651\n",
      "Name: pickupPrediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mstd = Man['pickups'].std()\n",
    "mmean = Man['pickups'].mean()\n",
    "bstd = Bronx['pickups'].std()\n",
    "bmean = Bronx['pickups'].mean()\n",
    "qstd = Queens['pickups'].std()\n",
    "qmean = Queens['pickups'].mean()\n",
    "brstd = Brooklyn['pickups'].std()\n",
    "brmean = Brooklyn['pickups'].mean()\n",
    "data['pickupPrediction'] = 0\n",
    "for index, row in data.iterrows():\n",
    "    if(row['Manhattan'] == 1):\n",
    "        if(row['pickups']  < (mmean - mstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (mmean + mstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Bronx'] == 1):\n",
    "        if(row['pickups']  < (bmean - bstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (bmean + bstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Queens'] == 1):\n",
    "        if(row['pickups']  < (qmean - qstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (qmean + qstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Brooklyn'] == 1):\n",
    "        if(row['pickups']  < (brmean - brstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (brmean + brstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "print(data['pickupPrediction'].describe())\n",
    "print(data['pickupPrediction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data\n",
    "y = data['pickupPrediction']\n",
    "del x['pickupPrediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Used\n",
    "<p> We believe using an f1-socre would be the best evaluation metric in our case. We need to take into account the false positive and false negative rate because our logisitc regression on the algorithm would always guesses one class which results in a precision of 100% for that class. We care about any form of misclassification of our pickup class prediction. We will give false negatives a higher cost because this means we have too few ubers in an area based off our preditcion. If we have a false positive then it means we have too many ubers in an area, which is bad, but not as bad as having too few. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "#                     ('clf', LogisticRegression())])\n",
    "\n",
    "# pipe_lr.fit(xTrain, yTrain)\n",
    "# y_pred = pipe_lr.predict(xTest)\n",
    "# print(classification_report(yTest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "We choose to use continuous test and training sets as our uber pickups were given hourly over a 6 month span. Our algorithm in the real world would be getting the data in hourly and then have to predict based off this new hourly data. We split our data into 10 folds splitting our data by the time that it occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "      <th>hday</th>\n",
       "      <th>Bronx</th>\n",
       "      <th>Brooklyn</th>\n",
       "      <th>Manhattan</th>\n",
       "      <th>Queens</th>\n",
       "      <th>is_morning</th>\n",
       "      <th>is_afternoon</th>\n",
       "      <th>is_evening</th>\n",
       "      <th>is_night</th>\n",
       "      <th>is_monday</th>\n",
       "      <th>is_tuesday</th>\n",
       "      <th>is_wednesday</th>\n",
       "      <th>is_thursday</th>\n",
       "      <th>is_friday</th>\n",
       "      <th>is_saturday</th>\n",
       "      <th>is_sunday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "      <td>17372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>820.426606</td>\n",
       "      <td>6.000039</td>\n",
       "      <td>8.820027</td>\n",
       "      <td>47.489005</td>\n",
       "      <td>1017.812802</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>2.536496</td>\n",
       "      <td>0.038453</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250058</td>\n",
       "      <td>0.208381</td>\n",
       "      <td>0.166705</td>\n",
       "      <td>0.374856</td>\n",
       "      <td>0.143679</td>\n",
       "      <td>0.143679</td>\n",
       "      <td>0.138153</td>\n",
       "      <td>0.143449</td>\n",
       "      <td>0.143679</td>\n",
       "      <td>0.143679</td>\n",
       "      <td>0.143679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1179.027565</td>\n",
       "      <td>3.706364</td>\n",
       "      <td>2.443018</td>\n",
       "      <td>19.772345</td>\n",
       "      <td>7.783546</td>\n",
       "      <td>0.018834</td>\n",
       "      <td>0.092917</td>\n",
       "      <td>0.220437</td>\n",
       "      <td>4.520593</td>\n",
       "      <td>0.192292</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433025</td>\n",
       "      <td>0.433058</td>\n",
       "      <td>0.406163</td>\n",
       "      <td>0.372723</td>\n",
       "      <td>0.484100</td>\n",
       "      <td>0.350775</td>\n",
       "      <td>0.350775</td>\n",
       "      <td>0.345071</td>\n",
       "      <td>0.350540</td>\n",
       "      <td>0.350775</td>\n",
       "      <td>0.350775</td>\n",
       "      <td>0.350775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>991.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>1012.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>367.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>789.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1023.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7883.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>1043.400000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickups           spd           vsb          temp           slp  \\\n",
       "count  17372.000000  17372.000000  17372.000000  17372.000000  17372.000000   \n",
       "mean     820.426606      6.000039      8.820027     47.489005   1017.812802   \n",
       "std     1179.027565      3.706364      2.443018     19.772345      7.783546   \n",
       "min        0.000000      0.000000      0.000000      2.000000    991.400000   \n",
       "25%       86.000000      3.000000      9.100000     31.500000   1012.400000   \n",
       "50%      367.000000      6.000000     10.000000     45.000000   1018.200000   \n",
       "75%      789.250000      8.000000     10.000000     64.000000   1023.000000   \n",
       "max     7883.000000     21.000000     10.000000     89.000000   1043.400000   \n",
       "\n",
       "              pcp01         pcp06         pcp24            sd          hday  \\\n",
       "count  17372.000000  17372.000000  17372.000000  17372.000000  17372.000000   \n",
       "mean       0.003821      0.026074      0.091036      2.536496      0.038453   \n",
       "std        0.018834      0.092917      0.220437      4.520593      0.192292   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.051667      3.166667      0.000000   \n",
       "max        0.280000      1.240000      2.100000     19.000000      1.000000   \n",
       "\n",
       "              Bronx      Brooklyn     Manhattan        Queens    is_morning  \\\n",
       "count  17372.000000  17372.000000  17372.000000  17372.000000  17372.000000   \n",
       "mean       0.250000      0.250000      0.250000      0.250000      0.250058   \n",
       "std        0.433025      0.433025      0.433025      0.433025      0.433058   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.250000      0.250000      0.250000      0.250000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       is_afternoon    is_evening      is_night     is_monday    is_tuesday  \\\n",
       "count  17372.000000  17372.000000  17372.000000  17372.000000  17372.000000   \n",
       "mean       0.208381      0.166705      0.374856      0.143679      0.143679   \n",
       "std        0.406163      0.372723      0.484100      0.350775      0.350775   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       is_wednesday   is_thursday     is_friday   is_saturday     is_sunday  \n",
       "count  17372.000000  17372.000000  17372.000000  17372.000000  17372.000000  \n",
       "mean       0.138153      0.143449      0.143679      0.143679      0.143679  \n",
       "std        0.345071      0.350540      0.350775      0.350775      0.350775  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reset_index(inplace=True, drop=True)\n",
    "y.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The data was already in order by hour. Each four rows represented one boroughs uber and weather data. We split our data into 10 folds for training. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1579 1580 1581] TEST: [1582 1583 1584 ... 3158 3159 3160]\n",
      "TRAIN: [   0    1    2 ... 3158 3159 3160] TEST: [3161 3162 3163 ... 4737 4738 4739]\n",
      "TRAIN: [   0    1    2 ... 4737 4738 4739] TEST: [4740 4741 4742 ... 6316 6317 6318]\n",
      "TRAIN: [   0    1    2 ... 6316 6317 6318] TEST: [6319 6320 6321 ... 7895 7896 7897]\n",
      "TRAIN: [   0    1    2 ... 7895 7896 7897] TEST: [7898 7899 7900 ... 9474 9475 9476]\n",
      "TRAIN: [   0    1    2 ... 9474 9475 9476] TEST: [ 9477  9478  9479 ... 11053 11054 11055]\n",
      "TRAIN: [    0     1     2 ... 11053 11054 11055] TEST: [11056 11057 11058 ... 12632 12633 12634]\n",
      "TRAIN: [    0     1     2 ... 12632 12633 12634] TEST: [12635 12636 12637 ... 14211 14212 14213]\n",
      "TRAIN: [    0     1     2 ... 14211 14212 14213] TEST: [14214 14215 14216 ... 15790 15791 15792]\n",
      "TRAIN: [    0     1     2 ... 15790 15791 15792] TEST: [15793 15794 15795 ... 17369 17370 17371]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "for train_index, test_index in tscv.split(x.values,y.values):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 1/5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we compare now to sklearn?\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSci\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "#From Sklearn documentation \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = list(unique_labels(y_true, y_pred))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None,\n",
    "                act_func='sigmoid',cost_func='quadratic',layers=2):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.act_func = act_func\n",
    "        self.cost_func = cost_func\n",
    "        self.layers = layers\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        W = []\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        W.append(W1)\n",
    "        for i in range(0,self.layers-2):\n",
    "            W1_num_elems = (self.n_hidden + 1)*self.n_hidden\n",
    "            W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "            W1 = W1.reshape(self.n_hidden, self.n_hidden + 1) # reshape to be W\n",
    "            W.append(W1)\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        W.append(W2)\n",
    "        return W\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        if(self.cost_func == 'quadratic'):\n",
    "            cost = np.mean((Y_enc-A3)**2)\n",
    "        else:\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        if(self.act_func == 'sigmoid'):\n",
    "            A2 = self._sigmoid(Z1)\n",
    "        else:\n",
    "            A2 = Z1\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        ones = np.ones(self.n_hidden + 1)\n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "#             print('a3',a3.shape)\n",
    "#             print('y',y.shape)\n",
    "            if(self.cost_func == 'quadratic'):\n",
    "                dJ_dz2 = -2*(y - a3)*a3*(1-a3)\n",
    "            else:\n",
    "                dJ_dz2 = a3-y\n",
    "            #loop here? but then a2 never changes \n",
    "            if(self.act_func == 'sigmoid'):\n",
    "                dJ_dz1 = dJ_dz2 @ W2 @ np.diag(a2*(1-a2))\n",
    "            else:\n",
    "                dJ_dz1 = dJ_dz2 @ W2 @ np.diag(ones)\n",
    "                # print(dJ_dz1.shape)\n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ a2[np.newaxis,:]\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ a1[np.newaxis,:] \n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        for i in range(0,len(self.W)-1):\n",
    "            if(i>0):\n",
    "                X = A3.T\n",
    "            _, _, _, _, A3 = self._feedforward(X,self.W[i], self.W[i+1])\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "            # feedforward all instances\n",
    "            W1 = []\n",
    "            W2 = []\n",
    "            A1arr = []\n",
    "            A2arr = []\n",
    "            A3arr = []\n",
    "            Z1arr = []\n",
    "            Z2arr = []\n",
    "            X_data = X.copy()\n",
    "            for i2 in range(0,len(self.W)-1):\n",
    "                W1 = self.W[i2]\n",
    "                W2 = self.W[i2+1]\n",
    "                if(i2>0):\n",
    "                    X_data = A3.T\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data,W1,W2)\n",
    "                A1arr.append(A1)\n",
    "                A2arr.append(A2)\n",
    "                A3arr.append(A3)\n",
    "                Z1arr.append(Z1)\n",
    "                Z2arr.append(Z2)\n",
    "            cost = self._cost(A3,Y_enc,W1,W2)\n",
    "            self.cost_.append(cost)\n",
    "            for i2 in range(len(self.W)-1,0,-1):\n",
    "                # compute gradient via backpropagation\n",
    "                print(i2)\n",
    "                if(i2 < len(self.W)-1):\n",
    "                    Y_enc = A3arr[i2-1]\n",
    "                print(Y_enc)\n",
    "                ## return an array of gradients to update weight array\n",
    "                ## iterate inside of get gradient function instead\n",
    "                grad1, grad2 = self._get_gradient(A1=A1arr[i2-1], A2=A2arr[i2-1], A3=A3arr[i2-1], Z1=Z1arr[i2-1],\n",
    "                                                  Z2=Z2arr[i2-1]\n",
    "                                                  ,Y_enc=Y_enc, W1=self.W[i2-1], W2=self.W[i2])\n",
    "                self.W[i2-1] -= self.eta * grad1\n",
    "                self.W[i2] -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50, \n",
    "                C=0.1,\n",
    "                epochs=50,\n",
    "                eta=0.001,\n",
    "                random_state=1,\n",
    "                act_func='sigmoid',\n",
    "                cost_func='reag',\n",
    "                layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "2\n",
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "1\n",
      "[[0.19226049 0.33873069 0.64819516 ... 0.19226049 0.36782159 0.74366688]\n",
      " [0.09635857 0.58109012 0.72602907 ... 0.09635857 0.19523399 0.04389513]\n",
      " [0.26181301 0.48856364 0.22499308 ... 0.26181301 0.4736671  0.26038765]\n",
      " ...\n",
      " [0.67463544 0.73598046 0.87826558 ... 0.67463544 0.58375395 0.45636033]\n",
      " [0.98182177 0.97609647 0.99862538 ... 0.98182177 0.96841328 0.97794939]\n",
      " [0.99895704 0.97857473 0.99526761 ... 0.99895704 0.99948101 0.99695669]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (50,1582) (3,1582) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-485-67a61e6e0051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from sklearn.metrics import f1_score\\nfrom sklearn.metrics import accuracy_score\\nnn = TwoLayerPerceptron(**params)\\ni = 1\\nfor train_index, test_index in tscv.split(x.values,y.values):\\n    print('fold', i)\\n    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\\n    nn.fit(X_train, y_train, print_progress=10)\\n    yhat = nn.predict(X_test)\\n    class_names = ['low','medium','high']\\n    plot_confusion_matrix(y_test, yhat, classes=class_names,\\n                      title='SciKit Learn Implementation')\\n    print('Test acc:',f1_score(y_test,yhat,average=None))\\n    i +=1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</anaconda/lib/python3.6/site-packages/decorator.py:decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-483-1bceae53656f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mZ1arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mZ2arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-430-b34a81a3abfb>\u001b[0m in \u001b[0;36m_cost\u001b[0;34m(self, A3, Y_enc, W1, W2)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_enc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_enc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mL2_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_L2_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mL2_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (50,1582) (3,1582) "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "nn = TwoLayerPerceptron(**params)\n",
    "i = 1\n",
    "for train_index, test_index in tscv.split(x.values,y.values):\n",
    "    print('fold', i)\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    nn.fit(X_train, y_train, print_progress=10)\n",
    "    yhat = nn.predict(X_test)\n",
    "    class_names = ['low','medium','high']\n",
    "    plot_confusion_matrix(y_test, yhat, classes=class_names,\n",
    "                      title='SciKit Learn Implementation')\n",
    "    print('Test acc:',f1_score(y_test,yhat,average=None))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
