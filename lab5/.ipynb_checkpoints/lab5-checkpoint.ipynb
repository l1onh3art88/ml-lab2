{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Our task for this lab is to create our own logistic regression model which is able to classify how many Uber pickups there will be (low, medium, or high) based off of different information in our dataset. The dataset is a collection of information about Uber pickups like time and location, joined with other data such as the weather for that time and location, what borough it is in, and whether or not it was a NYC public holiday. We split our predictions up by borough because certain boroughs like Manhattan generally always have a higher volume of pickups than boroughs like the Bronx, so aggregate predictions over all of NYC would not have been very insightful. Instead, we make predictions specific to each borough, with the exception of EWR and Staten Island, which we threw out because they did not contain enough data to make accurate predictions. We denote a \"high\" amount of pickups as greater than half a standard deviation above the mean for that borough. A \"low\" amount is less than half a standard deviation below the mean for that borough. A \"medium\" amount is inbetween. \n",
    "\n",
    "Our prediction task is valuable because it gives Uber insight into the time periods where they can be most profitable, and time periods where they can save money. For example, on New Years Eve there is most likely an extreme surge in the number of rides requested. If there are not enough drivers to satisfy all of these rides, people will go to Lyft or even just hail a yellow cab. However, if they prepare for this surge by incentivising drivers with an extra percentage of the ride money, there will be more drivers to satisfy the extra rides requests. Our model's insights would help pull in more profits and increases market share compared to treating every day and location as equally profitable. In production, this model would provide the best results if it were deployed so that it would run constantly and react to changing weather conditions, social movements, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/uber_nyc_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "      <td>29101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>490.215903</td>\n",
       "      <td>5.984924</td>\n",
       "      <td>8.818125</td>\n",
       "      <td>47.669042</td>\n",
       "      <td>30.823065</td>\n",
       "      <td>1017.817938</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.090464</td>\n",
       "      <td>2.529169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>995.649536</td>\n",
       "      <td>3.699007</td>\n",
       "      <td>2.442897</td>\n",
       "      <td>19.814969</td>\n",
       "      <td>21.283444</td>\n",
       "      <td>7.768796</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.093125</td>\n",
       "      <td>0.219402</td>\n",
       "      <td>4.520325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>991.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1012.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>449.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1022.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>2.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7883.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1043.400000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickups           spd           vsb          temp          dewp  \\\n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000   \n",
       "mean     490.215903      5.984924      8.818125     47.669042     30.823065   \n",
       "std      995.649536      3.699007      2.442897     19.814969     21.283444   \n",
       "min        0.000000      0.000000      0.000000      2.000000    -16.000000   \n",
       "25%        1.000000      3.000000      9.100000     32.000000     14.000000   \n",
       "50%       54.000000      6.000000     10.000000     46.000000     30.000000   \n",
       "75%      449.000000      8.000000     10.000000     64.500000     50.000000   \n",
       "max     7883.000000     21.000000     10.000000     89.000000     73.000000   \n",
       "\n",
       "                slp         pcp01         pcp06         pcp24            sd  \n",
       "count  29101.000000  29101.000000  29101.000000  29101.000000  29101.000000  \n",
       "mean    1017.817938      0.003830      0.026129      0.090464      2.529169  \n",
       "std        7.768796      0.018933      0.093125      0.219402      4.520325  \n",
       "min      991.400000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%     1012.500000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%     1018.200000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%     1022.900000      0.000000      0.000000      0.050000      2.958333  \n",
       "max     1043.400000      0.280000      1.240000      2.100000     19.000000  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> checking for nan or null values in the dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found only Borough has nan values so we remove the nan rows </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough       True\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_dt    False\n",
       "borough      False\n",
       "pickups      False\n",
       "spd          False\n",
       "vsb          False\n",
       "temp         False\n",
       "dewp         False\n",
       "slp          False\n",
       "pcp01        False\n",
       "pcp06        False\n",
       "pcp24        False\n",
       "sd           False\n",
       "hday         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that most of our data didnt have much correlation except temperate and the dew point temperature. We decided to get rid of this variable becasue it seemed very similar to temperature and did not think it would impact the machine learning. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickups</th>\n",
       "      <th>spd</th>\n",
       "      <th>vsb</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>slp</th>\n",
       "      <th>pcp01</th>\n",
       "      <th>pcp06</th>\n",
       "      <th>pcp24</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pickups</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.015708</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.002821</td>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.009676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spd</th>\n",
       "      <td>0.009741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.097041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vsb</th>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.086177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.047834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.063692</td>\n",
       "      <td>-0.296126</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>-0.545558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dewp</th>\n",
       "      <td>0.040082</td>\n",
       "      <td>-0.321606</td>\n",
       "      <td>-0.231294</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.489372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slp</th>\n",
       "      <td>-0.015708</td>\n",
       "      <td>-0.092761</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>-0.224537</td>\n",
       "      <td>-0.311156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.121508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp01</th>\n",
       "      <td>0.005007</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.488407</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp06</th>\n",
       "      <td>-0.002821</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.118346</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>0.128064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.039943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcp24</th>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.014408</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>-0.009676</td>\n",
       "      <td>0.097041</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.545558</td>\n",
       "      <td>-0.489372</td>\n",
       "      <td>0.121508</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>0.069664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickups       spd       vsb      temp      dewp       slp     pcp01  \\\n",
       "pickups  1.000000  0.009741 -0.008429  0.063692  0.040082 -0.015708  0.005007   \n",
       "spd      0.009741  1.000000  0.086177 -0.296126 -0.321606 -0.092761 -0.000357   \n",
       "vsb     -0.008429  0.086177  1.000000  0.025214 -0.231294  0.167039 -0.488407   \n",
       "temp     0.063692 -0.296126  0.025214  1.000000  0.896544 -0.224537 -0.013343   \n",
       "dewp     0.040082 -0.321606 -0.231294  0.896544  1.000000 -0.311156  0.115399   \n",
       "slp     -0.015708 -0.092761  0.167039 -0.224537 -0.311156  1.000000 -0.089752   \n",
       "pcp01    0.005007 -0.000357 -0.488407 -0.013343  0.115399 -0.089752  1.000000   \n",
       "pcp06   -0.002821  0.016668 -0.118346 -0.037295  0.013293 -0.104940  0.128064   \n",
       "pcp24   -0.022935 -0.010412  0.000895 -0.014408  0.001519 -0.134689  0.000997   \n",
       "sd      -0.009676  0.097041 -0.047834 -0.545558 -0.489372  0.121508  0.000310   \n",
       "\n",
       "            pcp06     pcp24        sd  \n",
       "pickups -0.002821 -0.022935 -0.009676  \n",
       "spd      0.016668 -0.010412  0.097041  \n",
       "vsb     -0.118346  0.000895 -0.047834  \n",
       "temp    -0.037295 -0.014408 -0.545558  \n",
       "dewp     0.013293  0.001519 -0.489372  \n",
       "slp     -0.104940 -0.134689  0.121508  \n",
       "pcp01    0.128064  0.000997  0.000310  \n",
       "pcp06    1.000000  0.251166  0.039943  \n",
       "pcp24    0.251166  1.000000  0.069664  \n",
       "sd       0.039943  0.069664  1.000000  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['dewp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We made the holiday column count 1 for yes and 0 for no. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hday'] = data['hday'].apply(lambda x: 0 if x=='N' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We one hot encoded our boroughs becuase they were string values </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotCols = pd.get_dummies(data['borough'])\n",
    "data = data.join(oneHotCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['borough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the time of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We based our hour groups by sunrise and sunset. Night is the time when the sun is down, which on average is from 8pm to 6am. Morning is from 6am till noon. Afternoon is from noon till 5pm. Evening is from 5pm till 8pm, which is around when the sunsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dateTest = data['pickup_dt'][0]\n",
    "print(int(dateTest[11:13]))\n",
    "data['is_morning'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 6 and int(x[11:13]) < 12) else 0)\n",
    "data['is_afternoon'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 12 and int(x[11:13]) < 17) else 0)\n",
    "data['is_evening'] = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 17 and int(x[11:13]) < 21) else 0)\n",
    "data['is_night']  = data['pickup_dt'].apply(lambda x: 1 if (int(x[11:13]) >= 21 or int(x[11:13]) < 6) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 hot encoding the weekday\n",
    "\n",
    "The weekday from the pickup_dt feature has been 1 hot encoded into monday-sunday. We believe having each day as a feature will help classify & predict the number of ubers necessary at a future date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "data['is_monday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 0 else 0)\n",
    "data['is_tuesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 1 else 0)\n",
    "data['is_wednesday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 2 else 0)\n",
    "data['is_thursday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 3 else 0)\n",
    "data['is_friday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 4 else 0)\n",
    "data['is_saturday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 5 else 0)\n",
    "data['is_sunday'] = data['pickup_dt'].apply(lambda x: 1 if datetime.date(int(x[0:4]),int(x[5:7]),int(x[8:10])).weekday() == 6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['pickup_dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We found that the borough EWR averages about 2.4 pickups every 96 hours so we are getting rid of the EWR borough from our dataset. We found that the borough Staten Island averages 1.6 pickups and hour and had a max 13 pickups in an hour over 6 months so we got rid of it from our dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           pickups     EWR\n",
      "count  4343.000000  4343.0\n",
      "mean      0.024177     1.0\n",
      "std       0.160937     0.0\n",
      "min       0.000000     1.0\n",
      "25%       0.000000     1.0\n",
      "50%       0.000000     1.0\n",
      "75%       0.000000     1.0\n",
      "max       2.000000     1.0\n",
      "           pickups  Staten Island\n",
      "count  4343.000000         4343.0\n",
      "mean      1.601888            1.0\n",
      "std       1.640451            0.0\n",
      "min       0.000000            1.0\n",
      "25%       0.000000            1.0\n",
      "50%       1.000000            1.0\n",
      "75%       2.000000            1.0\n",
      "max      13.000000            1.0\n"
     ]
    }
   ],
   "source": [
    "d1 = data.where(data['EWR']==1)[['pickups','EWR']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data.EWR == 0]\n",
    "del data['EWR']\n",
    "d1 = data.where(data['Staten Island']==1)[['pickups','Staten Island']]\n",
    "print(d1.describe())\n",
    "d1 = d1.dropna()\n",
    "data = data[data['Staten Island'] == 0]\n",
    "del data['Staten Island']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We have three cateogries of pickup traffic low, medium, high. We found these by finding the mean and standard deviation of each borough. Low is half a standard deviation below the mean and high is half a stadard deviation above the mean. Anything else is counted as a medium amount of pickups. This means that the category of low, medium, and high pickup amount depends on the borough. If we did base our categories by borough then Manhattan would always be in the high pickup amount category and Queens would always be in the low pickup amount category.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4343.000000\n",
      "mean     2387.253281\n",
      "std      1434.724668\n",
      "min         0.000000\n",
      "25%      1223.500000\n",
      "50%      2269.000000\n",
      "75%      3293.500000\n",
      "max      7883.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean       50.667050\n",
      "std        31.029223\n",
      "min         0.000000\n",
      "25%        29.000000\n",
      "50%        46.000000\n",
      "75%        66.000000\n",
      "max       262.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      309.354824\n",
      "std       154.368300\n",
      "min         0.000000\n",
      "25%       196.000000\n",
      "50%       308.000000\n",
      "75%       410.000000\n",
      "max       831.000000\n",
      "Name: pickups, dtype: float64\n",
      "count    4343.000000\n",
      "mean      534.431269\n",
      "std       294.810182\n",
      "min         0.000000\n",
      "25%       331.500000\n",
      "50%       493.000000\n",
      "75%       675.000000\n",
      "max      2009.000000\n",
      "Name: pickups, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Man = data[data.Manhattan == 1]\n",
    "Bronx = data[data.Bronx == 1]\n",
    "Queens = data[data.Queens == 1]\n",
    "Brooklyn = data[data.Brooklyn == 1]\n",
    "\n",
    "print(Man['pickups'].describe())\n",
    "print(Bronx['pickups'].describe())\n",
    "print(Queens['pickups'].describe())\n",
    "print(Brooklyn['pickups'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Using the mean and stadard deviation of each borough to place each row of data into a category. We can see that our categories are almost fairly balanced within each borough.  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    17372.000000\n",
      "mean         0.935816\n",
      "std          0.771724\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          2.000000\n",
      "max          2.000000\n",
      "Name: pickupPrediction, dtype: float64\n",
      "1    6955\n",
      "0    5766\n",
      "2    4651\n",
      "Name: pickupPrediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mstd = Man['pickups'].std()\n",
    "mmean = Man['pickups'].mean()\n",
    "bstd = Bronx['pickups'].std()\n",
    "bmean = Bronx['pickups'].mean()\n",
    "qstd = Queens['pickups'].std()\n",
    "qmean = Queens['pickups'].mean()\n",
    "brstd = Brooklyn['pickups'].std()\n",
    "brmean = Brooklyn['pickups'].mean()\n",
    "data['pickupPrediction'] = 0\n",
    "for index, row in data.iterrows():\n",
    "    if(row['Manhattan'] == 1):\n",
    "        if(row['pickups']  < (mmean - mstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (mmean + mstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Bronx'] == 1):\n",
    "        if(row['pickups']  < (bmean - bstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (bmean + bstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Queens'] == 1):\n",
    "        if(row['pickups']  < (qmean - qstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (qmean + qstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "    if(row['Brooklyn'] == 1):\n",
    "        if(row['pickups']  < (brmean - brstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',0)\n",
    "        elif(row['pickups'] > (brmean + brstd/2)):\n",
    "            data.set_value(index,'pickupPrediction',2)\n",
    "        else:\n",
    "            data.set_value(index,'pickupPrediction',1)\n",
    "print(data['pickupPrediction'].describe())\n",
    "print(data['pickupPrediction'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True, drop=True)\n",
    "for index, row in data.iterrows():\n",
    "    if(index < 12):\n",
    "        continue\n",
    "    if(data['pickupPrediction'][index-4] == 0):        \n",
    "        data.set_value(index,'1hrAgoLow',1)\n",
    "        data.set_value(index,'1hrAgoMed',0)\n",
    "        data.set_value(index,'1hrAgoHigh',0)\n",
    "    elif(data['pickupPrediction'][index-4] == 1):        \n",
    "        data.set_value(index,'1hrAgoLow',0)\n",
    "        data.set_value(index,'1hrAgoMed',1)\n",
    "        data.set_value(index,'1hrAgoHigh',0)\n",
    "    else:\n",
    "        data.set_value(index,'1hrAgoLow',0)\n",
    "        data.set_value(index,'1hrAgoMed',0)\n",
    "        data.set_value(index,'1hrAgoHigh',1)\n",
    "        \n",
    "    if(data['pickupPrediction'][index-8] == 0):        \n",
    "        data.set_value(index,'2hrAgoLow',1)\n",
    "        data.set_value(index,'2hrAgoMed',0)\n",
    "        data.set_value(index,'2hrAgoHigh',0)\n",
    "    elif(data['pickupPrediction'][index-8] == 1):        \n",
    "        data.set_value(index,'2hrAgoLow',0)\n",
    "        data.set_value(index,'2hrAgoMed',1)\n",
    "        data.set_value(index,'2hrAgoHigh',0)\n",
    "    else:\n",
    "        data.set_value(index,'2hrAgoLow',0)\n",
    "        data.set_value(index,'2hrAgoMed',0)\n",
    "        data.set_value(index,'2hrAgoHigh',1)\n",
    "        \n",
    "    if(data['pickupPrediction'][index-12] == 0):        \n",
    "        data.set_value(index,'3hrAgoLow',1)\n",
    "        data.set_value(index,'3hrAgoMed',0)\n",
    "        data.set_value(index,'3hrAgoHigh',0)\n",
    "    elif(data['pickupPrediction'][index-12] == 1):        \n",
    "        data.set_value(index,'3hrAgoLow',0)\n",
    "        data.set_value(index,'3hrAgoMed',1)\n",
    "        data.set_value(index,'3hrAgoHigh',0)\n",
    "    else:\n",
    "        data.set_value(index,'3hrAgoLow',0)\n",
    "        data.set_value(index,'3hrAgoMed',0)\n",
    "        data.set_value(index,'3hrAgoHigh',1)\n",
    "data = data.iloc[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data\n",
    "y = data['pickupPrediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del x['pickupPrediction']\n",
    "del x['pickups']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# x = x.values #returns a numpy array\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x.iloc[:,0:8].values)\n",
    "x_not_sclaed = x.iloc[:,8:].values\n",
    "x = np.concatenate((x_scaled, x_not_sclaed), axis=1)\n",
    "x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Used\n",
    "<p> We believe using an f1-socre would be the best evaluation metric in our case. We care about the misclassification of our data because misclassifying an amount of ubers for an area would either lead to there being too many or few drivers. This would either result in wasting drivers time or missing out rides from uber users. F1-score takes into account the false positive and false negative, which represents both cases mentioned. We will be taking the f1-score of each class and then averaging the f1-scores. Earlier, we used this dataset for logisitc regression and found that it frequently only guessed one class. Averaging the f1-scores of each class would punish the classifier for guessing only class and limit it to an average f1-score of 33%.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "We choose to use continuous test and training sets as our uber pickups were given hourly over a 6 month span. Our algorithm in the real world would be getting the data in hourly and then have to predict based off this new hourly data. We split our data into 6 folds splitting our data by the time that it occured. This represents us retraining our algorithm every month off new data. We believe that this would help avoid having to retrain the model frequently and keep up with the new season's new data. This would be crucial becuase we do not have a full year's amount of data. When we got a full year's amount of data, we would use a 12 fold split to represent each month of the year. Also, we believe it would be best for the Uber to only keep the last year's data for the model. This is because the avergage amount of uber trips could change yearly and keeping old data would drag down the means and standard deviations we computed above. Overall, we thing using a continuous time series split helps represent the model's training and usage in the real world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The data was already in order by hour. Each four rows represented one boroughs uber and weather data. We split our data into 10 folds for training. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 2477 2478 2479] TEST: [2480 2481 2482 ... 4957 4958 4959]\n",
      "TRAIN: [   0    1    2 ... 4957 4958 4959] TEST: [4960 4961 4962 ... 7437 7438 7439]\n",
      "TRAIN: [   0    1    2 ... 7437 7438 7439] TEST: [7440 7441 7442 ... 9917 9918 9919]\n",
      "TRAIN: [   0    1    2 ... 9917 9918 9919] TEST: [ 9920  9921  9922 ... 12397 12398 12399]\n",
      "TRAIN: [    0     1     2 ... 12397 12398 12399] TEST: [12400 12401 12402 ... 14877 14878 14879]\n",
      "TRAIN: [    0     1     2 ... 14877 14878 14879] TEST: [14880 14881 14882 ... 17357 17358 17359]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "for train_index, test_index in tscv.split(x.values,y.values):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 1/10, random_state = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
