{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team:\n",
    "    -Robbie Keehan\n",
    "    -Max Goldstein\n",
    "    -Spencer Chang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not to use Deepdream \n",
    "\n",
    "When analyzing a CNN visually, there are a few techniques that give valuable insights into what the CNN has learnt. One of these is by starting with a picuture of random noise or a completely empty image. Then that image is fed through the already trained network and instead of updating the layers, the layer stays the same and the input image is iteratiely updated based off of gradient ascent. One appraoch to this is to see what the network expects a specific class to look like. This gives very valuable insights into what the network expects a class to look like, which can allow a human to decide whether or not the networks ideal version of a class is what it should be. Another approach is to do something similar, but see what the output is at every layer, which is called DeepDream. DeepDream offers artist value in creating weird morphed images, but it actually can provide helpful insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pattern(layer_name, image, size=150):\n",
    "    # Build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered.\n",
    "    \n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = 0\n",
    "    loss = (K.abs(layer_output))\n",
    "    # Compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "    # Normalization trick: we normalize the gradient\n",
    "    grads /= K.maximum(K.mean(K.abs(grads)),1e-7)\n",
    "\n",
    "    # This function returns the loss and grads given the input picture\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    \n",
    "    # We start from a random image selected\n",
    "    input_img_data = image\n",
    "\n",
    "    # Run gradient ascent for 40 steps\n",
    "    step = .001\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "#         print(loss_value)\n",
    "        input_img_data += step * grads_value \n",
    "        \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(76, 114), (152, 229), (183, 275)]\n",
      "<PIL.Image.Image image mode=RGB size=114x76 at 0x65690AA10>\n",
      "<PIL.Image.Image image mode=RGB size=229x152 at 0x65B0822D0>\n",
      "<PIL.Image.Image image mode=RGB size=275x183 at 0x657066710>\n"
     ]
    }
   ],
   "source": [
    "img_url = 'grass.jpeg'\n",
    "\n",
    "# We preprocess the image into a 4D tensor\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "layer = 'block4_conv1'\n",
    "\n",
    "def load_image_as_array(url):\n",
    "#     response = requests.get(url)\n",
    "    img = Image.open(img_url)\n",
    "    img = np.array(img).astype(float)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img /= 255.\n",
    "    return img\n",
    "\n",
    "img = load_image_as_array(img_url)\n",
    "shapes = img.shape[1:3]\n",
    "new_shapes = [shapes]\n",
    "\n",
    "for i in range (1,3):\n",
    "    new_shape = tuple( [int(shapes[0]/(1.2*i)), int(shapes[1]/(1.2*i))])\n",
    "    new_shapes.append(new_shape)\n",
    "    \n",
    "new_shapes = new_shapes[::-1]\n",
    "print(new_shapes)\n",
    "small_og_image = resize_img(img,new_shapes[0])\n",
    "mega_ganglife_og = np.copy(img)\n",
    "for idx,size in enumerate(new_shapes):\n",
    "    # Save Last dreamed image\n",
    "    if(idx+1 == len(new_shapes)):\n",
    "        img = resize_img(img,size)\n",
    "        img = generate_pattern(layer,img)\n",
    "        img = img[np.newaxis,...].astype('float32')\n",
    "        saved_img = Image.fromarray(img[0].astype('uint8'), 'RGB')\n",
    "        print(saved_img)\n",
    "        saved_img.save('dream{}.jpeg'.format(idx))\n",
    "        continue\n",
    "    # Resize Image Down\n",
    "    img = resize_img(img,size)\n",
    "    \n",
    "    # Dream\n",
    "    img = generate_pattern(layer,img)\n",
    "    \n",
    "    # add axis\n",
    "    img = img[np.newaxis,...].astype('float32')\n",
    "    \n",
    "    # Save Image\n",
    "    saved_img = Image.fromarray(img[0].astype('uint8'), 'RGB')\n",
    "    saved_img.save('dream{}.jpeg'.format(idx))\n",
    "    print(saved_img)\n",
    "    \n",
    "    # Upscale Dreamed Image\n",
    "    img = resize_img(img,new_shapes[idx+1]) \n",
    "    \n",
    "    # Upscale downscaled original image\n",
    "    upsized_small_og = resize_img(small_og_image,new_shapes[idx+1]) # small image resized up\n",
    "    \n",
    "    # downscale original image\n",
    "    downsize_og = resize_img(mega_ganglife_og,new_shapes[idx+1]) # original image resized down\n",
    "    \n",
    "    # find lost frequencies at given size\n",
    "    lost_frequencies = downsize_og - upsized_small_og # lost detail\n",
    "    # add back frequencies to dreamed image\n",
    "    img += lost_frequencies \n",
    "    img /= 255.\n",
    "   \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining model choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Deepdream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using L1 gradient normalization for gradient updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding random shifts/resizing in the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and using deep dream process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Credit: using another type of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
